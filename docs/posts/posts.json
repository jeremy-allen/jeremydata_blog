[
  {
    "path": "posts/2021-04-12-using-bash-to-analyze-your-r-scripts/",
    "title": "Using Bash to Discover All Hex Codes in All of Your R Scripts",
    "description": "How to analyze all of your R and Rmarkdown files at once using the bash grep utility in a terminal",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-04-12",
    "categories": [
      "tidyverse",
      "dataviz",
      "bash",
      "programming"
    ],
    "contents": "\nProblem: how do I identify specific elements used in all of my .R and .Rmd files, for example, how do I count all the hex codes I’ve ever used?\nDuring a recent dataviz challenge, Ijeamaka Anyene manually counted all the hex codes in her R scripts. She wondered how to do it automatically. I’m no Bash expert, but I knew the gist of a solution, so I googled around a few places and was able to piece together a Bash command using grep and sed.\nThe solution\n\n\n\nThis is a series of three commands, using grep, sort, and sed, that you run in a Mac or Linux terminal. That terminal is most likely using a Bash or Zsh shell. grep, sort, and sed are three utility “apps” that work in Bash and Zsh.\ngrep\ngrep is a search utility, and here we tell grep to search my directory to find a pattern. The pattern is \\\"#[a-zA-Z0-9]{6}\\\". That will locate all instances of hex codes like “#FFFFFF”. The --include parameters limit the search to .R and .Rmd files. The -Eroh flags tell grep to (r) recursively search all sub-folders in my directory, enable (E) extended regex capability so we can use the {6} modifier in our pattern, (h) hide filenames in the results because we only want the hex codes, and show (o) only the match in the results instead of the default which is to show the whole line where are a match occurs. The . that appears after the pattern is the file path where we want grep to search. The . stands for the current dirctory, so make sure that your terminal is set to the directory where all of your scripts live, or you can type another file path there.\n|\nThe vertical pipe lets us pass the results of the grep search on to another utility, in this case, the sort utility because I want the grep search results sorted, but this is not necessary. Then we use the | again to pass the results to the sed utility so we can remove the quotation marks that were captured with each hexcode.\nsort and sed\nThe sed command is s/\"//g. The s means we want to make a substitution. We want to substitute all quotation marks with nothing, i.e., delete all quotation marks. Between the first two slashes we provide the pattern for sed to find, in our case, a quotation mark. Between the second and third slash we provide the replacement, in our case, nothing, so there is nothing between the second and third slash. After the third slash we tell sed where to perform the substitution, in our case that is throughout all the search results, so we use g, which I think means global.\nwrite to file\nFinally, we want our results written into a text document, so we use > instead of | to write our results into a new file called my_colors.txt.\nThen we can import that file into R suing read_lines() and count our use of each hexcode.\nDon’t forget\nThe whole command is:grep -Eroh --include=\"*.Rmd\" --include=\"*.R\" \"\\\"#[a-zA-Z0-9]{6}\\\"\" . | sort | sed 's/\"//g' > my_colors.txt\nCheck out the nice dataviz Ijeamaka Anyene made!\n\n\n\n",
    "preview": "posts/2021-04-12-using-bash-to-analyze-your-r-scripts/grep_hexcodes.png",
    "last_modified": "2021-04-12T12:47:05-04:00",
    "input_file": {},
    "preview_width": 1206,
    "preview_height": 2024
  },
  {
    "path": "posts/2021-03-20-video-game-title-emotions-a-text-analysis/",
    "title": "Video Game Title Emotions - a text analysis",
    "description": "Using R to create a data visualization of the emotional content of video game titles",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-03-20",
    "categories": [
      "tidyverse",
      "dataviz",
      "ggplot2",
      "tidytext"
    ],
    "contents": "\n\n\n\nFigure 1: Video Game Title Emotions\n\n\n\nVideo Game Title Emotions - a text analysis\nThis week’s Tidy Tuesday challenge offered video game data for analysis. I analyzed the text in the video game titles. Using the NRC Emotion Lexicon I measured the joy words and sad words in the titles by year.\nI used the tidytext package to tokenize each years-worth of game titles and remove stop words and plotted the polar plots with the gpglot2 package. Then I made word clouds using wordcloud2. The clowplot package was used to place screenshots of the wordclouds within the final image.\nCode is also on GitHub\n\n\nlibrary(wordcloud2)\nlibrary(cowplot)\nlibrary(tidyverse)\nlibrary(tidytext)\n\n\nget_sentiments(\"nrc\")\n\ndata(stop_words)\n\nnrc_types <- get_sentiments(\"nrc\") %>% \n pull(sentiment) %>% \n unique()\n\nnrc_joy <- get_sentiments(\"nrc\") %>% \n filter(sentiment == \"joy\")\nnrc_sad <- get_sentiments(\"nrc\") %>% \n filter(sentiment == \"sadness\")\n\nstops <- stop_words %>% \n bind_rows(tibble(\n  word = c(\"edition\", \"online\", \"ii\", \"iii\", \"iv\", \"vi\", \"vii\",\n           \"viii\", \"ix\", \"definitive\", \"remastered\"),\n  lexicon = c(rep(\"jda\", 11))\n ))\n\n\n\n\n\ngames <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-16/games.csv')\n\n\n\n\n\n# function to get emotion words and output as list of two types\nget_emotion_words <- function(x) {\n \n name <- as_tibble(x) %>% \n  rename(gamename = value) %>% \n  count(gamename) %>% \n  mutate(gamename = str_remove_all(gamename, \"<U.....>\")) %>% \n  select(-n) %>%\n  unnest_tokens(\n   output = word,\n   input = gamename,\n   token = \"words\",\n   to_lower = TRUE,\n   strip_punct = TRUE\n  ) %>%\n  anti_join(stops) %>% \n  filter(\n   nchar(word) > 1,              # must be more than one character\n   str_detect(word, \"^[a-z]\"),   # must start with a letter\n   str_detect(word, \"[a-z]\")     # must not be only numbers\n  ) %>% \n  count(word, sort = TRUE) %>%\n  mutate(word = reorder(word, n))\n \n emotion_words <- list(\n  joy = name %>% inner_join(nrc_joy),\n  sad = name %>% inner_join(nrc_sad)\n )\n \n emotion_words\n \n}\n\nname_emotions <- games %>%\n filter(year > 2014 & year < 2021) %>% \n group_by(year) %>% \n summarise(sum_avg_players = sum(avg),\n           emotion_words = get_emotion_words(gamename)) %>% \n ungroup() %>%\n arrange(year)\n\nemotions_scored <- name_emotions %>% \n unnest(emotion_words) %>% \n mutate(score = n*sum_avg_players) %>% \n group_by(year, sentiment) %>% \n summarise(valence = sum(score)) %>% \n ungroup() %>% \n mutate(sentiment = factor(sentiment, levels = c(\"sadness\", \"joy\"), order = TRUE))\n\n\n\n\n\njoy_title_words <- name_emotions %>%\n unnest(emotion_words) %>%\n select(word, sentiment, freq = n) %>% \n filter(sentiment == \"joy\") %>% \n select(-sentiment) %>%\n group_by(word) %>% \n summarise(freq = sum(freq))\n\nsad_title_words <- name_emotions %>%\n unnest(emotion_words) %>%\n select(word, sentiment, freq = n) %>% \n filter(sentiment == \"sadness\") %>%\n select(-sentiment) %>%\n group_by(word) %>% \n summarise(freq = sum(freq))\n\n\n\n\n\n# open these in the viewer pane then screenshot and add screenshot to directory\njoy_cloud <- wordcloud2(\n data = joy_title_words,\n size = .7,\n gridSize = 1,\n color = \"#e600e6\",\n backgroundColor = \"black\",\n minRotation = 0,\n maxRotation = 0,\n rotateRatio = 0,\n shape = \"circle\"\n)\n\nsad_cloud <- wordcloud2(\n data = sad_title_words,\n size = 1.1,\n gridSize = 1,\n color = \"#625062\",\n backgroundColor = \"black\",\n minRotation = 0,\n maxRotation = 0,\n rotateRatio = 0,\n shape = \"circle\"\n)\n\n\n\n\n\np <- emotions_scored %>%\n ggplot(aes(x = sentiment, y = valence, fill = sentiment)) +\n geom_bar(stat = \"identity\", width = 1) +\n scale_fill_manual(values = c(\"#e600e6\", \"#625062\"), breaks = c(\"joy\", \"sadness\")) +\n labs(x = \"\",\n      y = \"\",\n      title = \"Emotional Content of Words in Video Game Titles\nIn all years, game titles consisted of more sad words than joy words.\",\n      subtitle = \"Emotional valence of words determined by the NRC Emotion Lexicon\",\n      caption = \"Data source: SteamCharts | Data viz: @jeremy_data\nReference: Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon, Saif Mohammad and Peter Turney, In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California.\",\n      fill = \"\") +\n coord_polar() +\n facet_wrap(~ year, ncol = 6) +\n theme_minimal() +\n theme(\n  text = element_text(color = \"#cfafcf\"),\n  strip.text = element_text(size = 12, color = \"#cfafcf\"),\n  plot.title = element_text(size = 34, margin = margin(0,0,.25,0, unit = \"cm\")),\n  plot.subtitle = element_text(size = 24, margin = margin(.5,0,4,0, unit = \"cm\")),\n  plot.caption = element_text(size = 14, margin = margin(24,0,0,0, unit = \"cm\")),\n  legend.box.margin = margin(.25,0,2,0, unit = \"cm\"),\n  legend.position = \"top\",\n  legend.text = element_text(size = 16),\n  legend.spacing.x = unit(1, 'cm'),\n  legend.text.align = 0,\n  panel.grid = element_blank(),\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  axis.text.x = element_blank(),\n  axis.text.y = element_blank(),\n  plot.margin = margin(3,3,.5,3, unit = \"cm\"),\n  plot.background = element_rect(fill = \"black\"),\n ) \n\nq <- ggdraw(p) +\n draw_image(here::here(\"joycloud.png\"), x = .125, y = .265, hjust = 0, vjust = 0, width = 0.25, height = 0.15) +\n draw_image(here::here(\"sadcloud.png\"), x = .48, y = .12, hjust = 0, vjust = 0, width = 0.37, height = 0.37)\n\nggsave(\"sad_games.png\", plot = q, width = 30, height = 26, units = \"in\")\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-20-video-game-title-emotions-a-text-analysis/title_emotions.png",
    "last_modified": "2021-03-20T13:51:54-04:00",
    "input_file": {},
    "preview_width": 3632,
    "preview_height": 2500
  },
  {
    "path": "posts/2021-03-13-counts-and-cumulative-sums-by-group/",
    "title": "Counts and Cumulative Sums by Group",
    "description": "In a time series, how do I add a counter and cumulative sum that resets every 30 minutes?",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-03-13",
    "categories": [
      "data.table",
      "time series"
    ],
    "contents": "\nProblem:\nIn a time series, how do I add a counter and cumulative sum that resets every 30 minutes?\nI keep forgetting about R’s seq_len() function. I’m writing about it here, so when I inevitably google for it I can find it here. Let’s see how it works.\nWe will be using data.table for this example.\n\n\nlibrary(data.table)\nlibrary(lubridate)\n\n\n#---- fake data ----\n\ntimes <- seq.POSIXt(\n from = as.POSIXct(\"2021-03-12 08:00:00\"),\n to =   as.POSIXct(\"2021-03-12 12:00:00\"),\n by = \"sec\"\n)\n\naccounts <- seq(1111, 9999, by = 1000)\n\namounts <- seq(1, 5000, by = 1)\n\nset.seed(865)\n\ndt <- data.table(\n datetime = sample(times, 1500, replace = TRUE),\n account = sample(accounts, 1500, replace = TRUE),\n amount = sample(amounts, 1500, replace = TRUE)\n)\n\n# order by account and datetime\nsetkeyv(dt, c(\"account\", \"datetime\"))\n\n\n\nn = seq_len(.N) By Group Row Number in data.table\nAfter making our fake time series data, we can uses seq_len(.N) along with the by argument in data.table to add a counter that resets for each 30-minute group. Notice the transformation of a variable within the by = of the data.table.\nCumulative sum by 30-minute periods\n\n\n#---- counts and sums ----\n\n# count and sum by half hour using seq_len() to show \n# each observation's position within its 30-minute group\ndt[,\n   `:=`(n_by_30_mins = seq_len(.N), cumulative_amount = cumsum(amount)) ,\n   by = .(account, floor_date(datetime, \"30 minutes\"))]\n\nhead(dt, 50)\n\n\n               datetime account amount n_by_30_mins cumulative_amount\n 1: 2021-03-12 08:00:00    1111   3798            1              3798\n 2: 2021-03-12 08:02:06    1111    303            2              4101\n 3: 2021-03-12 08:03:55    1111   1620            3              5721\n 4: 2021-03-12 08:04:10    1111   4926            4             10647\n 5: 2021-03-12 08:04:24    1111   3112            5             13759\n 6: 2021-03-12 08:07:02    1111   3515            6             17274\n 7: 2021-03-12 08:08:06    1111   1767            7             19041\n 8: 2021-03-12 08:09:02    1111   1411            8             20452\n 9: 2021-03-12 08:12:12    1111   1082            9             21534\n10: 2021-03-12 08:14:26    1111   2164           10             23698\n11: 2021-03-12 08:16:49    1111   3050           11             26748\n12: 2021-03-12 08:19:03    1111   4329           12             31077\n13: 2021-03-12 08:21:25    1111   4462           13             35539\n14: 2021-03-12 08:23:53    1111    969           14             36508\n15: 2021-03-12 08:25:41    1111   4287           15             40795\n16: 2021-03-12 08:31:25    1111   2323            1              2323\n17: 2021-03-12 08:33:29    1111   3629            2              5952\n18: 2021-03-12 08:34:25    1111   2697            3              8649\n19: 2021-03-12 08:34:57    1111   2878            4             11527\n20: 2021-03-12 08:35:41    1111   2921            5             14448\n21: 2021-03-12 08:36:12    1111   3286            6             17734\n22: 2021-03-12 08:38:18    1111   2393            7             20127\n23: 2021-03-12 08:40:44    1111   2245            8             22372\n24: 2021-03-12 08:43:19    1111   3969            9             26341\n25: 2021-03-12 08:44:26    1111   3517           10             29858\n26: 2021-03-12 08:44:28    1111    291           11             30149\n27: 2021-03-12 08:45:12    1111   1545           12             31694\n28: 2021-03-12 08:45:30    1111   2679           13             34373\n29: 2021-03-12 08:47:30    1111    114           14             34487\n30: 2021-03-12 08:48:18    1111    958           15             35445\n31: 2021-03-12 08:48:45    1111   4566           16             40011\n32: 2021-03-12 08:50:03    1111   3496           17             43507\n33: 2021-03-12 08:50:55    1111   1823           18             45330\n34: 2021-03-12 08:51:06    1111   4881           19             50211\n35: 2021-03-12 08:54:06    1111     35           20             50246\n36: 2021-03-12 08:55:10    1111    484           21             50730\n37: 2021-03-12 08:57:55    1111   2192           22             52922\n38: 2021-03-12 09:01:45    1111    461            1               461\n39: 2021-03-12 09:02:30    1111   4523            2              4984\n40: 2021-03-12 09:04:10    1111   2560            3              7544\n41: 2021-03-12 09:05:29    1111   1884            4              9428\n42: 2021-03-12 09:06:18    1111   4933            5             14361\n43: 2021-03-12 09:07:30    1111    100            6             14461\n44: 2021-03-12 09:07:52    1111     99            7             14560\n45: 2021-03-12 09:10:21    1111   2865            8             17425\n46: 2021-03-12 09:13:05    1111   2584            9             20009\n47: 2021-03-12 09:14:44    1111   3649           10             23658\n48: 2021-03-12 09:18:51    1111   1139           11             24797\n49: 2021-03-12 09:19:13    1111   2582           12             27379\n50: 2021-03-12 09:23:20    1111   2875           13             30254\n               datetime account amount n_by_30_mins cumulative_amount\n\n\n\n\n",
    "preview": "posts/2021-03-13-counts-and-cumulative-sums-by-group/counts.png",
    "last_modified": "2021-03-13T17:23:13-05:00",
    "input_file": {},
    "preview_width": 1766,
    "preview_height": 1130
  },
  {
    "path": "posts/2021-03-12-by-row-in-base-r-tidyverse-and-datatable/",
    "title": "By Row in Base R, Tidyverse, and data.table",
    "description": "Which rows have more 1s than 0s?.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-03-12",
    "categories": [
      "rowwise",
      "tidyverse",
      "data.table"
    ],
    "contents": "\nPROBLEM: Add a column that indicates if a row has more 1s than 0s, use 1 if true, 0 if false, but only consider specific columns and allow for NAs.\nThis question was asked on Twitter, and I want to elaborate on the solutions here.\nWe know we need to include\na column selection constraint\nproperly handle NAs\nanswers in a new column\n1 for TRUE and 0 for FALSE\nFirst, let’s make sample data that includes a row with NA.\n\n\n# PROBLEM: add a column that indicates if a row has more 1s than 0s,\n# use 1 if true, 0 if false, but only consider certain columns and\n# allow for NAs.\n\n# sample data\ndf <- data.frame(\n  a = as.integer(c(1,0,1,0,0)),\n  b = as.integer(c(1,0,1,0,0)),\n  c = as.integer(c(1,0,1,1,0)),\n  d = as.integer(c(1,1,0,9,0)),\n  e = as.integer(c(1,1,0,1,0)),\n  f = as.integer(c(1,1,NA,0,0)),\n  g = as.integer(c(1,1,1,0,0))\n)\n\n\n\nLet’s try a base R solution first. Base R has a function rowMeans() that could be helpful.\n\n\n#---- base R solution ----\n\n# base R rowMeans() says yes to row 4, but row 4 does not have \n# more 1s than 0s, and it will fail on any non-numeric columns\ndf$more_1s <- ifelse(rowMeans(df, na.rm = T) > .5,1,0)\n\ndf\n\n\n  a b c d e  f g more_1s\n1 1 1 1 1 1  1 1       1\n2 0 0 0 1 1  1 1       1\n3 1 1 1 0 0 NA 1       1\n4 0 0 1 9 1  0 0       1\n5 0 0 0 0 0  0 0       0\n\nHowever, if use the mean of each row, instead of literally just counting 1s and 0s, then we can be fooled by rows with larger numbers, like row 4 above. So, let’s make a function that only counts 1s and 0s, which is the problem we were asked to solve.\n\n\n# function to count only ones and zeros and report TRUE or\n# FALSE if more 1s, and convert the logical to integer\nis_more_1s <- function(x) {\n  as.integer(\n    sum(x == 1, na.rm = T) > sum(x == 0, na.rm = T)\n  )\n}\n\n# this gets row 4 correct\ndf$more_1s <- apply(df, 1, is_more_1s)\n\ndf\n\n\n  a b c d e  f g more_1s\n1 1 1 1 1 1  1 1       1\n2 0 0 0 1 1  1 1       1\n3 1 1 1 0 0 NA 1       1\n4 0 0 1 9 1  0 0       0\n5 0 0 0 0 0  0 0       0\n\nThis function only counts 1s and 0s, ignoring the 9 in row 4 and therefore giving us the correct answer in our new column. The function also returns a 1 if TRUE and a 0 if FALSE.\nlibrary(tidyverse)\nLet’s try a tidyverse solution that uses rowwise() instead of apply()\n\n\n#---- tidyverse solution ----\n\n# sample data\ndf <- data.frame(\n  a = as.integer(c(1,0,1,0,0)),\n  b = as.integer(c(1,0,1,0,0)),\n  c = as.integer(c(1,0,1,1,0)),\n  d = as.integer(c(1,1,0,9,0)),\n  e = as.integer(c(1,1,0,1,0)),\n  f = as.integer(c(1,1,NA,0,0)),\n  g = as.integer(c(1,1,1,0,0))\n)\n\nlibrary(tidyverse)\n\ntb <- as_tibble(df)\ntb %>% \n  rowwise() %>% \n  mutate(more_1s = is_more_1s(c_across(a:g))) # only a through g\n\n\n# A tibble: 5 x 8\n# Rowwise: \n      a     b     c     d     e     f     g more_1s\n  <int> <int> <int> <int> <int> <int> <int>   <int>\n1     1     1     1     1     1     1     1       1\n2     0     0     0     1     1     1     1       1\n3     1     1     1     0     0    NA     1       1\n4     0     0     1     9     1     0     0       0\n5     0     0     0     0     0     0     0       0\n\nHere, rowwise() makes sure we are counting across rows, and c_across() lets us constrain which columns we consider, which is part of the problem we were asked to solve.\nlibrary(data.table)\nUsing only data.table we don’t have rowwise() from dplyr, so we use base R’s apply() again. We use .SD and .SDcols to specify which columns we want to be constrained to.\n\n\n#---- data.table solution ----\n\n# sample data\ndf <- data.frame(\n  a = as.integer(c(1,0,1,0,0)),\n  b = as.integer(c(1,0,1,0,0)),\n  c = as.integer(c(1,0,1,1,0)),\n  d = as.integer(c(1,1,0,9,0)),\n  e = as.integer(c(1,1,0,1,0)),\n  f = as.integer(c(1,1,NA,0,0)),\n  g = as.integer(c(1,1,1,0,0))\n)\n\nlibrary(data.table)\n\ndt <- as.data.table(df)\nmy_cols <- letters[1:7] # only a through g\ndt[, more_1s := apply(.SD, 1, is_more_1s), .SDcols = my_cols]\n\ndt\n\n\n   a b c d e  f g more_1s\n1: 1 1 1 1 1  1 1       1\n2: 0 0 0 1 1  1 1       1\n3: 1 1 1 0 0 NA 1       1\n4: 0 0 1 9 1  0 0       0\n5: 0 0 0 0 0  0 0       0\n\n\n\n\n",
    "preview": "posts/2021-03-12-by-row-in-base-r-tidyverse-and-datatable/solution.png",
    "last_modified": "2021-04-12T13:20:06-04:00",
    "input_file": "by-row-in-base-r-tidyverse-and-datatable.utf8.md",
    "preview_width": 396,
    "preview_height": 260
  },
  {
    "path": "posts/2021-02-24-covid-19-death-tracking-app/",
    "title": "COVID-19 Death Tracking App",
    "description": "An R Shiny app designed for my family to track a single metric and to promote data literacy by including explanatory text with the data visualizations.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-02-24",
    "categories": [
      "shiny",
      "web scraping",
      "plotly",
      "ggplot2"
    ],
    "contents": "\n\n\n\nFigure 1: Top distinguishing words in men’s and women’s Christian-Living book descriptions\n\n\n\nI made this R Shiny app for my family to track a single metric. To promote data literacy, I included explanatory text with the data visualizations. CSS flexbox controls the layout, so the app works on mobile devices well.\nThe main visualization is an animated chart I created with the plotly package for R. Plotly for R enables interactive javascript-based visuals. I chose this for the main data visual in order to make obvious when the coronavirus is spreading exponentially and when it is not.\nThe image above is just a screenshot. Go check out the app at https://jeremyallen.shinyapps.io/covid_deaths/.\n\n\n\n",
    "preview": "posts/2021-02-24-covid-19-death-tracking-app/app_screenshot.png",
    "last_modified": "2021-02-24T09:59:00-05:00",
    "input_file": {},
    "preview_width": 3522,
    "preview_height": 2146
  },
  {
    "path": "posts/2021-02-22-web-scraping-and-text-analysis-of-gendered-christian-book-descriptions/",
    "title": "Use R and tidytext to Scrape and Analyze Word Variance in Gendered Christian Book Descriptions",
    "description": "How does word usage vary in Christian book descriptions marketed distinctly for men and women?",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-02-22",
    "categories": [
      "web scraping",
      "tidyverse",
      "tidytext",
      "tidytext",
      "nlp"
    ],
    "contents": "\n\n\n\nFigure 1: Top distinguishing words in men’s and women’s Christian-Living book descriptions\n\n\n\nThe Question: How does word usage vary in Christian book descriptions marketed distinctly for men and women?\nIn this article I primarily use the tidytext and tidyverse packages to analyze Christian book descriptions. The NRC Word-Emotion Association Lexicon allows us to analyze various kinds of emotional words. The amazing polite package ensures we are courteous when we scrape the descriptions from the website. The rvest package processes all of the HTML for us. I also use the showtext package to bring in Google fonts.\n\n\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(lubridate)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(rvest)\nlibrary(polite)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(assertthat)\nlibrary(showtext)\n\nfont_add(\"cardo-r\", regular = \"Cardo-Regular.ttf\")\nfont_add(\"cardo-b\", regular = \"Cardo-Bold.ttf\")\nfont_add(\"cardo-i\", regular = \"Cardo-Italic.ttf\")\n\n\n\nAfter loading the packages we bow() to our webpages then scrape() their contents. The christianbook.com website has subsections for books marketed at men and women separately, and each section contains the same number of books, giving us a nice balance. The first pages to scrape are their search results pages for men’s and women’s books. From those we will extract links to the individual book pages. but first, the main two results pages.\n\n\ndate_of_data <- Sys.Date()\n\n# URLs where book search results are\nurl_women <- \"https://www.christianbook.com/page/christian-living/christian-living-for-women?view_all=1\"\nurl_men   <- \"https://www.christianbook.com/page/christian-living/christian-living-for-men?view_all=1\"\n\n# Create sessions using bow() from the polite package, then scrape each page.\nsession_women <- bow(url_women, force = TRUE)\nsession_men   <- bow(url_men, force = TRUE)\npage_women    <- scrape(session_women)\npage_men      <- scrape(session_men)\n\n# Create a function that will extract each book's URL from the search results page,\n# then use the function to extract the URLs.\nget_cb_book_urls <- function(page) {\n  \n  page %>% \n    html_nodes(\"a.CBD-PreviewGroupItemImage\") %>%\n    html_attr(\"href\") %>% \n    as_tibble() %>% \n    rename(url = value) %>% \n    # paste domain onto each URL\n    mutate(url = glue(\"https://www.christianbook.com{url}\")) %>% \n    drop_na()\n  \n}\nbooks_women <- get_cb_book_urls(page_women)\nbooks_men   <- get_cb_book_urls(page_men)\n\n# Test that we have at least one URL in each.\nassert_that(nrow(books_women) > 0, msg = \"No URLs for women's books were found in books_women\")\nassert_that(nrow(books_men)   > 0, msg = \"No URLs for men's books were found in books_men\")\n\n\n\nNow that we have tibbles that contain URLs for all the books, we can add a gender column to each then combine them into a single tibble. The next challenge is defining a function to visit each book’s page and scrape its description and metadata such as author and title. Most of these books have two descriptions, one from the publisher and one that is not labeled, so I am not sure where it comes from. We want both descriptions. We end up with a few that have no description, so we drop them.\n\n\n# Add gender and date metadata, then combine into a single tibble.\nbooks_women <- books_women %>% \n  mutate(gender = \"women\",\n         date_scraped = Sys.Date())\nbooks_men   <- books_men %>% \n  mutate(gender = \"men\",\n         date_scraped = Sys.Date())\nbooks_all <- bind_rows(books_women, books_men)\n\n# Create a function to scrape descriptions from each book page.\nget_book_data <- function(x) {\n  \n  desc_session <- bow(x)\n  desc_page    <- scrape(desc_session)\n  \n  desc1 <- desc_page %>% \n    html_nodes(\".CBD-PD-Publisher-Description\") %>%\n    html_text() %>% \n    str_replace_all(\"\\\\u0092\", \"'\") %>% \n    str_replace_all(\"\\\\u0085\", \"...\") %>% \n    str_replace_all(\"([a-z])([A-Z])\", \"\\\\1 \\\\2\") %>%\n    str_remove_all(\"\\\\n\") %>% \n    str_replace_all(\"\\\\s{2,4}\", \" \")\n  \n  if(!length(desc1) > 0){\n    desc1 <- \" \"\n  }\n  \n  desc2 <- desc_page %>% \n    html_nodes(\".CBD-PD-Description\") %>%\n    html_text() %>% \n    str_replace_all(\"\\\\u0092\", \"'\") %>% \n    str_replace_all(\"\\\\u0085\", \"...\") %>% \n    str_replace_all(\"([a-z])([A-Z])\", \"\\\\1 \\\\2\") %>% \n    str_remove_all(\"\\\\n\") %>% \n    str_replace_all(\"\\\\s{2,4}\", \" \")\n  \n  if(!length(desc2) > 0){\n    desc2 <- \" \"\n  }\n  \n  description <- str_c(desc1, desc2, sep = \" \")\n  \n  title <- desc_page %>% \n    html_nodes(\".CBD-ProductDetailTitle\") %>%\n    html_text() %>%\n    pluck(1)\n  \n  sku <- desc_page %>% \n    html_nodes(\".CBD-ProductDetail-Medium.CBD-ProductDetailSKU\") %>%\n    html_text() %>%\n    str_extract(\"(?<=: ).*$\")\n  \n  tmp_author <- desc_page %>% \n    html_nodes(\"a.CBD-ProductDetailAuthorLink\") %>%\n    html_text()\n  \n  if(length(tmp_author) > 0){\n    author <- tmp_author %>% pluck(1)\n  } else {\n    author <- \"No Author\"\n  }\n  \n  # Combine elements into a tibble and return.\n  tibble(\n    sku = sku,\n    author = author,\n    title = title,\n    description = description\n  )\n\n}\n\n\n# Scrape the description from each individual book page then unnest() into columns.\n# There will be a 5-second pause between each scrape, so this can take a while.\nbooks_all <- books_all %>% \n  mutate(book_data = map(url, get_book_data)) %>% \n  unnest(book_data)\n\n# Do we have any without a word boundary, i.e., empty? If so, remove.\nempties <- books_all %>% filter(!str_detect(description, \"\\\\b\"))\nbooks_all <- books_all %>% filter(str_detect(description, \"\\\\b\"))\n\nassert_that(nrow(books_all) >= 1, msg = \"WARNING! Looks like you ended up with no rows in books_all.\")\n\ncloseAllConnections()\n# Clean garbage\ngc()\n\n\n\nMaking plots!\nWe will start by looking at the most frequently used words, but our real goal is to see which words most distinguish between the two kinds of book descriptions. First we need to clean the text and remove words that interfere with the purpose of our analysis.\n\n\ntheme_set(theme_fivethirtyeight())\n          \nsummary(books_all)\n\n# Make a vector of author names. We will try text analysis with and without\n# author names in the descriptions.\nauthors <- sort(unique(books_all$author)) %>% str_to_lower()\nnames <- authors %>%\n  str_split(\" \") %>%\n  unlist() %>% \n  str_remove_all(\"[:punct:]\") %>%\n  tibble(name = .) %>% \n  filter(str_detect(name, \"[a-z]{3,}\")) %>%\n  arrange(name) %>% \n  pull(name) %>% \n  unique()\n\n# Make a vector of stop words, words we want to remove from the descriptions \n# because they are meaningless for our analysis or because their inclusion masks\n# or dominates the findings we are interested in. These words tend to be about \n# a book itself not about the content of the book. Combine with names and stop words.\nmy_drops <- c(\"york\", \"selling\", \"copies\", \"sold\", \"online\", \"ebook\",\n           \"popular\", \"readers\", \"movie\", \"reader\", \"updates\", \"hardcover\",\n           \"padded\", \"packaging\", \"bestseller\", \"trademark\", \"excelente\",\n           \"faux\", \"cover\", \"portable\", \"makes\", \"includes\", \"leather\",\n           \"imitation\", \"ribbon\", \"marker\", \"including\", \"bestselling\",\n           \"plot\", \"text\", \"los\", \"las\", \"225ginas\", \"233cdotas\", \"del\", \"por\",\n           \"bible\", \"book\", \"god\", \"god's\", \"page\", \"pages\", \"readings\",\n           \"reading\", \"author\", \"updated\", \"ter\", \"keurst\")\nmy_drops <- append(my_drops, names)\nmy_stops <- tibble(word = my_drops,\n                   lexicon = \"jeremy\")\nall_stops <- bind_rows(stop_words, my_stops)\n\n# We also want to remove bibles.\nbibles <- c(\"NIV\", \"NLT\", \"KJV\", \"NKJV\", \"NKJ\", \"CSB\", \"NASB\",\n            \"GNT\", \"Translation\", \"ESV\")\nbooks_all <- books_all %>%\n  filter(str_detect(title, pattern = paste(bibles, collapse = \"|\"), negate = TRUE))\n\n# Make sure we still have books\nassert_that(nrow(books_all) >= 1, msg = \"WARNING! Looks like you ended up with no rows in books_all after dropping bibles.\")\n\n# Make counts for how many books per gender.\nn_men <- books_all %>% \n  count(gender, sort = TRUE) %>% \n  filter(gender == \"men\") %>% \n  pull(n)\nn_women <- books_all %>% \n  count(gender, sort = TRUE) %>% \n  filter(gender == \"women\") %>% \n  pull(n)\n\nwords_desc <- books_all %>% \n  unnest_tokens(word, description) %>%\n  anti_join(all_stops, by = \"word\") %>%\n  mutate(word = str_replace_all(word, \"^devotions$\", \"devotional\")) %>%\n  mutate(word = str_replace_all(word, \"^woman$\", \"women\")) %>%\n  mutate(word = str_replace_all(word, \"^man$\", \"men\")) %>%\n  mutate(word = str_remove_all(word, \"'s\")) %>% \n  filter(str_detect(word, \"[a-z]{3,}\")) %>%\n  count(gender, word, sort = TRUE) %>% \n  mutate(word = fct_reorder(word, n)) %>% \n  mutate(gender = factor(gender)) %>% \n  ungroup()\n\n#---- top words ----\n\n# Initiate using our custom font.\nshowtext_auto()\n  \nplot_desc <- words_desc %>% \n  group_by(gender) %>% \n  top_n(n = 20, wt = n) %>% \n  ungroup() %>% \n  mutate(gender = as.factor(gender),\n           word = reorder_within(word, n, gender)) %>%\n  ggplot(aes(word, n, fill = gender)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d(begin = .2,\n                       end = .6,\n                       direction = -1,\n                       option = \"B\",\n                       alpha = .85) +\n  scale_x_reordered() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Word Frequency in Women's and Men's Christian-Living Book Descriptions\",\n       subtitle = glue(\"{n_men} books for men, {n_women} books for women, no bibles\\nBook descriptions from christianbook.com, {date_of_data}\\nThe word 'god' dominated each category and was excluded\\nAuthor names also removed\"),\n       caption =  \"Data collection and analysis by @jeremy_data using R's tidytext package and NRC Word-Emotion Association Lexicon\\nReferences: 1. Silge J, Robinson D (2016). 'tidytext: Text Mining and Analysis Using Tidy Data Principles\\nin R.' JOSS, 1(3). doi: 10.21105/joss.00037\\n2. Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney,\\nComputational Intelligence, 29 (3), 436-465, 2013.\"\n       ) +\n  facet_wrap(~gender, ncol = 2, scales = \"free_y\") +\n  coord_flip() +\n  theme(\n    text = element_text(family = \"cardo-b\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\"),\n    plot.title = element_text(size = 18, margin = margin(1, 0, 10, 0, \"pt\")),\n    plot.subtitle = element_text(size = 12, margin = margin(1, 0, 75, 0, \"pt\")),\n    plot.caption = element_text(size = 10, margin = margin(50, 0, 1, 0, \"pt\")),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(0, \"pt\"),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 12, family = \"cardo-b\"),\n    panel.background = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 14, family = \"cardo-b\", margin = margin(5, 0, 15, 0, \"pt\"))\n  )\nplot_desc\n\nggsave(\"women_men_word_freq.png\",\n       plot = plot_desc,\n       path = \"output\",\n       width = 35, height = 25, units = \"cm\",  dpi = 300)\n\n\n\n\nAfter getting a sense of the most frequent words in both sets of book descriptions, we use TF-IDF to understand which words distinguish between the two sets best. The result is the large image that opens this article above, but I’ll show it again below.\n\n\n#---- top distinct words ----\n\n# Some words are common to descriptions for both male and female books.\n# Which words are most distinguishing between the two?\nwords_tfidf <- words_desc %>% \n  bind_tf_idf(word, gender, n) %>% \n  arrange(desc(tf_idf)) %>%\n  mutate(word = factor(word, levels = rev(unique(word)))) %>% \n  group_by(gender) %>% \n  top_n(n = 20, wt = tf_idf) %>% \n  ungroup() %>% \n  mutate(gender = as.factor(gender),\n           word = reorder_within(word, tf_idf, gender))\n  \n\nplot_tfidf <- words_tfidf %>% \n  ggplot(aes(word, tf_idf, fill = gender)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d(begin = .2,\n                       end = .6,\n                       direction = -1,\n                       option = \"B\",\n                       alpha = .85) +\n  scale_x_reordered() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Top Distinguishing Words in Christian-Living Book Descriptions\",\n       subtitle = glue(\"'Distinguishing' determined by term frequency-inverse document frequency\\nwhich means words that tend to appear in descriptions for one gender but not the other\\n{n_men} books for men, {n_women} books for women, no bibles\\nBook descriptions from christianbook.com, {date_of_data}\\nThe word 'god' dominated each category and was excluded\\nAuthor names also removed\"),\n       caption =  \"Data collection and analysis by @jeremy_data using R's tidytext package and NRC Word-Emotion Association Lexicon\\nReferences: 1. Silge J, Robinson D (2016). 'tidytext: Text Mining and Analysis Using Tidy Data Principles\\nin R.' JOSS, 1(3). doi: 10.21105/joss.00037\\n2. Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney,\\nComputational Intelligence, 29 (3), 436-465, 2013.\"\n       ) +\n  facet_wrap(~gender, ncol = 2, scales = \"free_y\") +\n  coord_flip() +\n  theme(\n    text = element_text(family = \"cardo-b\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\"),\n    plot.title = element_text(size = 18, margin = margin(1, 0, 10, 0, \"pt\")),\n    plot.subtitle = element_text(size = 12, margin = margin(1, 0, 75, 0, \"pt\")),\n    plot.caption = element_text(size = 10, margin = margin(50, 0, 1, 0, \"pt\")),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(0, \"pt\"),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 12, family = \"cardo-b\"),\n    panel.background = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 14, family = \"cardo-b\", margin = margin(5, 0, 15, 0, \"pt\"))\n  )\nplot_tfidf\n\n\nggsave(\"women_men_word_tfidf.png\",\n       plot = plot_tfidf,\n       path = \"output\",\n       width = 35, height = 25, units = \"cm\",  dpi = 300)\n\n\n\n\nEmotion Expression\nNow we can add emotional sentiment scores to our words and see how men’s and women’s Christian book descriptions express these emotions differently. This time I will lead with the images, and the code will follow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#---- joy words ----\n\njoy_words <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"joy\")\n\nplot_joy <- words_desc %>% \n  inner_join(joy_words) %>%\n  group_by(gender) %>% \n  top_n(n = 20, wt = n) %>% \n  ungroup() %>% \n  mutate(gender = as.factor(gender),\n           word = reorder_within(word, n, gender)) %>%\n  ggplot(aes(word, n, fill = gender)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d(begin = .2,\n                       end = .6,\n                       direction = -1,\n                       option = \"B\",\n                       alpha = .85) +\n  scale_x_reordered() +\n  labs(x = NULL,\n       y = NULL,\n       title =    \"Top 'Joy' Words in Christian-Living Book Descriptions\",\n       subtitle = glue(\"{n_men} books for men, {n_women} books for women, no bibles\\nBook descriptions from christianbook.com, {date_of_data}\\nThe word 'god' dominated each category and was excluded\\nAuthor names also removed\"),\n       caption =  \"Data collection and analysis by @jeremy_data using R's tidytext package and NRC Word-Emotion Association Lexicon\\nReferences: 1. Silge J, Robinson D (2016). 'tidytext: Text Mining and Analysis Using Tidy Data Principles\\nin R.' JOSS, 1(3). doi: 10.21105/joss.00037\\n2. Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney,\\nComputational Intelligence, 29 (3), 436-465, 2013.\"\n       ) +\n  facet_wrap(~gender, ncol = 2, scales = \"free_y\") +\n  coord_flip() +\n  theme(\n    text = element_text(family = \"cardo-b\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\"),\n    plot.title = element_text(size = 18, margin = margin(1, 0, 10, 0, \"pt\")),\n    plot.subtitle = element_text(size = 12, margin = margin(1, 0, 75, 0, \"pt\")),\n    plot.caption = element_text(size = 10, margin = margin(50, 0, 1, 0, \"pt\")),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(0, \"pt\"),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 12, family = \"cardo-b\"),\n    panel.background = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 14, family = \"cardo-b\", margin = margin(5, 0, 15, 0, \"pt\"))\n  )\nplot_joy\n\nggsave(\"women_men_word_joy.png\",\n       plot = plot_joy,\n       path = \"output\",\n       width = 35, height = 25, units = \"cm\",  dpi = 300)\n\n#---- negative words ----\n\nnegative_words <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"negative\")\n\nplot_negative <- words_desc %>% \n  inner_join(negative_words) %>%\n  group_by(gender) %>% \n  top_n(n = 20, wt = n) %>% \n  ungroup() %>% \n  mutate(gender = as.factor(gender),\n           word = reorder_within(word, n, gender)) %>%\n  ggplot(aes(word, n, fill = gender)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d(begin = .2,\n                       end = .6,\n                       direction = -1,\n                       option = \"B\",\n                       alpha = .85) +\n  scale_x_reordered() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Top 'Negative' Words in Christian-Living Book Descriptions\",\n       subtitle = glue(\"{n_men} books for men, {n_women} books for women, no bibles\\nBook descriptions from christianbook.com, {date_of_data}\\nThe word 'god' dominated each category and was excluded\\nAuthor names also removed\"),\n       caption =  \"Data collection and analysis by @jeremy_data using R's tidytext package and NRC Word-Emotion Association Lexicon\\nReferences: 1. Silge J, Robinson D (2016). 'tidytext: Text Mining and Analysis Using Tidy Data Principles\\nin R.' JOSS, 1(3). doi: 10.21105/joss.00037\\n2. Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney,\\nComputational Intelligence, 29 (3), 436-465, 2013.\"\n       ) +\n  facet_wrap(~gender, ncol = 2, scales = \"free_y\") +\n  coord_flip() +\n  theme(\n    text = element_text(family = \"cardo-b\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\"),\n    plot.title = element_text(size = 18, margin = margin(1, 0, 10, 0, \"pt\")),\n    plot.subtitle = element_text(size = 12, margin = margin(1, 0, 75, 0, \"pt\")),\n    plot.caption = element_text(size = 10, margin = margin(50, 0, 1, 0, \"pt\")),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(0, \"pt\"),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 12, family = \"cardo-b\"),\n    panel.background = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 14, family = \"cardo-b\", margin = margin(5, 0, 15, 0, \"pt\"))\n  )\nplot_negative\n\nggsave(\"women_men_word_negative.png\",\n       plot = plot_negative,\n       path = \"output\",\n       width = 35, height = 25, units = \"cm\",  dpi = 300)\n\n#---- positive words ----\n\npositive_words <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"positive\")\n\nplot_positive <- words_desc %>% \n  inner_join(positive_words) %>%\n  group_by(gender) %>% \n  top_n(n = 20, wt = n) %>% \n  ungroup() %>% \n  mutate(gender = as.factor(gender),\n           word = reorder_within(word, n, gender)) %>%\n  ggplot(aes(word, n, fill = gender)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d(begin = .2,\n                       end = .6,\n                       direction = -1,\n                       option = \"B\",\n                       alpha = .85) +\n  scale_x_reordered() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Top 'Positive' Words in Christian-Living Book Descriptions\",\n       subtitle = glue(\"{n_men} books for men, {n_women} books for women, no bibles\\nBook descriptions from christianbook.com, {date_of_data}\\nThe word 'god' dominated each category and was excluded\\nAuthor names also removed\"),\n       caption =  \"Data collection and analysis by @jeremy_data using R's tidytext package and NRC Word-Emotion Association Lexicon\\nReferences: 1. Silge J, Robinson D (2016). 'tidytext: Text Mining and Analysis Using Tidy Data Principles\\nin R.' JOSS, 1(3). doi: 10.21105/joss.00037\\n2. Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney,\\nComputational Intelligence, 29 (3), 436-465, 2013.\"\n       ) +\n  facet_wrap(~gender, ncol = 2, scales = \"free_y\") +\n  coord_flip() +\n  theme(\n    text = element_text(family = \"cardo-b\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\"),\n    plot.title = element_text(size = 18, margin = margin(1, 0, 10, 0, \"pt\")),\n    plot.subtitle = element_text(size = 12, margin = margin(1, 0, 75, 0, \"pt\")),\n    plot.caption = element_text(size = 10, margin = margin(50, 0, 1, 0, \"pt\")),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(0, \"pt\"),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 12, family = \"cardo-b\"),\n    panel.background = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 14, family = \"cardo-b\", margin = margin(5, 0, 15, 0, \"pt\"))\n  )\nplot_positive\n\nggsave(\"women_men_word_positive.png\",\n       plot = plot_positive,\n       path = \"output\",\n       width = 35, height = 25, units = \"cm\",  dpi = 300)\n\n#---- fear words ----\n\nfear_words <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"fear\")\n\nplot_fear <- words_desc %>% \n  inner_join(fear_words) %>%\n  group_by(gender) %>% \n  top_n(n = 20, wt = n) %>% \n  ungroup() %>% \n  mutate(gender = as.factor(gender),\n           word = reorder_within(word, n, gender)) %>%\n  ggplot(aes(word, n, fill = gender)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d(begin = .2,\n                       end = .6,\n                       direction = -1,\n                       option = \"B\",\n                       alpha = .85) +\n  scale_x_reordered() +\n  labs(x = NULL,\n       y = NULL,\n       title =    \"Top 'Fear' Words in Christian-Living Book Descriptions\",\n       subtitle = glue(\"{n_men} books for men, {n_women} books for women, no bibles\\nBook descriptions from christianbook.com, {date_of_data}\\nThe word 'god' dominated each category and was excluded\\nAuthor names also removed\"),\n       caption =  \"Data collection and analysis by @jeremy_data using R's tidytext package and NRC Word-Emotion Association Lexicon\\nReferences: 1. Silge J, Robinson D (2016). 'tidytext: Text Mining and Analysis Using Tidy Data Principles\\nin R.' JOSS, 1(3). doi: 10.21105/joss.00037\\n2. Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney,\\nComputational Intelligence, 29 (3), 436-465, 2013.\"\n       ) +\n  facet_wrap(~gender, ncol = 2, scales = \"free_y\") +\n  coord_flip() +\n  theme(\n    text = element_text(family = \"cardo-b\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\"),\n    plot.title = element_text(size = 18, margin = margin(1, 0, 10, 0, \"pt\")),\n    plot.subtitle = element_text(size = 12, margin = margin(1, 0, 75, 0, \"pt\")),\n    plot.caption = element_text(size = 10, margin = margin(50, 0, 1, 0, \"pt\")),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.ticks.length = unit(0, \"pt\"),\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 12, family = \"cardo-b\"),\n    panel.background = element_blank(),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 14, family = \"cardo-b\", margin = margin(5, 0, 15, 0, \"pt\"))\n  )\nplot_fear\n\nggsave(\"women_men_word_fear.png\",\n       plot = plot_fear,\n       path = \"output\",\n       width = 35, height = 25, units = \"cm\",  dpi = 300)\n\n\n\nAll the code in a single Rmarkdown doc is on my GitHub.\n\n\n\n",
    "preview": "posts/2021-02-22-web-scraping-and-text-analysis-of-gendered-christian-book-descriptions/women_men_word_tfidf.png",
    "last_modified": "2021-02-22T15:28:26-05:00",
    "input_file": {},
    "preview_width": 4133,
    "preview_height": 2952
  },
  {
    "path": "posts/2021-02-21-tidyverse-to-army-how-copy-over/",
    "title": "Tidyverse to Army. How Copy? Over.",
    "description": "Helping the Army rename their Confederate-named bases with some help from R's tidyverse, rvest, quanteda, and Shiny",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-02-21",
    "categories": [
      "shiny",
      "nlp",
      "rvest",
      "tidyverse",
      "web scraping",
      "purrr"
    ],
    "contents": "\n\nThe Biden administration has given the Army three years to rename Confederate-named bases. I built this little web app that scrapes all Union general names from Wikipedia, then identifies the first letter of their last name and how many syllables are in that name. Click on a Confederate-named base and see all Union generals who match the first letter and number of syllables. I even found some Google fonts that are similar to 19-century fonts.\nI use the rvest package to scrape the tables from Wikipedia. I count syllables with the nsyllable() function from the quanteda package. With purrr’s pmap() function I iterate over each Union general to make a Shiny UI element that formats each general’s name and ranks into a card format. All layout for the Shiny app is done with CSS Flexbox.\nSee the web app here.\nAll the code is on my GitHub.\n\n\n\n",
    "preview": "posts/2021-02-21-tidyverse-to-army-how-copy-over/website-screenshot.png",
    "last_modified": "2021-02-21T14:19:02-05:00",
    "input_file": {},
    "preview_width": 2268,
    "preview_height": 2522
  },
  {
    "path": "posts/2021-02-21-using-rs-ggplot2-to-recreate-a-hand-drawn-web-du-bois-data-visualization/",
    "title": "Using R's ggplot2 to recreate a hand-drawn W.E.B. Du Bois data visualization",
    "description": "fine-tuning margins, lables, and annotations",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-02-21",
    "categories": [
      "racism",
      "sociology",
      "dataviz",
      "ggplot2"
    ],
    "contents": "\n\n\n\nFigure 1: original and recreation\n\n\n\nFor the 1900 Paris Exposition, sociologists W.E.B Du Bois curated a set of photographs and created hand-drawn data visualizations to combat racism in science and culture. In February 2021 #TidyTuesday and the #DuBoisChallenge collaborated to challenge us to use modern tools to recreate Du Bois’s data visualizations.\nI chose to recreate plate 51. I learned a ton about spacing in R’s ggplot2 package. How to implement margins around any element, how to add empty lines in title and subtitles, how to use if statements to conditionally place data labels and more. All the code is on my Github.\nPlease visit two places to read and see more about W.E.B Du Bois’s work for the 1900 Paris Exposition. The Library of Congress site for all of the materials. Anthony Starks’s recreations of all the pieces.\n\n\n\n",
    "preview": "posts/2021-02-21-using-rs-ggplot2-to-recreate-a-hand-drawn-web-du-bois-data-visualization/before_after.png",
    "last_modified": "2021-02-21T20:54:11-05:00",
    "input_file": {},
    "preview_width": 2906,
    "preview_height": 1892
  },
  {
    "path": "posts/2021-02-16-scrape-hundreds-of-pdf-documents-from-the-web-with-r-and-rvest/",
    "title": "Scrape Hundreds of PDF Documents From the Web with R and rvest",
    "description": "Safely download all complaints, affidavits, and indictments for all capitol attackers",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-02-16",
    "categories": [
      "tidyverse",
      "web scraping",
      "rvest",
      "purrr",
      "polite"
    ],
    "contents": "\n\nProblem: How do I get all the complaints, affidavits, and indictments (all at once!) for everyone charged so far in the attack on the U.S. Capitol?\nSolution: R’s tidyverse is a good way. The rvest package has easy functions for scraping the web.\nIn this article I will use only tidyverse packages. I’ll show you exploratory data analysis, how to loop through an HTML table with the rvest and purrr packages, automatically create unique folders on your system for each set of documents, and use the amazing polite package to handle identifying ourselves to the web host and limiting our activity on their server so that we do not cause harm.\n\n\nlibrary(assertthat)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(polite)\nlibrary(fs)\n\n\n\nAll of the documents we want are on this page of the United States District Attorney’s Office for the District of Columbia. The page is mostly a big HTML table with a row for each defendant. A documents column contains all the documents for each person.\nUsing functions from the polite package, we start by creating a session then scraping the HTML from the webpage. This does not give us the PDF documents, though.\n\n\ncases_url <- \"https://www.justice.gov/usao-dc/capitol-breach-cases\"\n\n# from the polite package, we properly identify ourselves and respect any explicit limits\nsession <- bow(cases_url, force = TRUE)\n\n# scrape the page contents\ncases_page <- scrape(session)\n\n\n\nNow that we have the HTML content we do a little exploratory data analysis to see how everything is organized and decide how we want to download all the defendant documents.\nHow many cases do we have?\n\n\n# extract the HTML table from the page content\ncases_table <- cases_page %>% \n html_node(\"table\") %>% \n html_table()\n\n# How many cases do we have?\ncases_table %>%\n  pull(`Case Number`) %>%\n  n_distinct()\n\n\n[1] 159\n\nHow many defendants do we have?\n\n\n# How many defendants do we have?\ncases_table %>%\n  pull(`Name`) %>%\n  n_distinct()\n\n\n[1] 199\n\nIf any defendant names are duplicated, let’s see them.\n\n\n# if any defendant names are duplicated, let's see them\nif (cases_table %>%\n    pull(Name) %>%\n    duplicated() %>%\n    any()) {\n  \n  cases_table %>%\n    count(Name, sort = TRUE) %>% \n    filter(n > 1) %>% \n    arrange(desc(n))\n  \n}\n\n\n\nIf any case numbers are duplicated, let’s see them.\n\n\n# if any case numbers are duplicated, let's see them\nif (cases_table %>%\n    pull(`Case Number`) %>%\n    duplicated() %>%\n    any()) {\n  \n  cases_table %>% \n    group_by(`Case Number`) %>% \n    mutate(n = n()) %>% \n    ungroup() %>% \n    select(n, `Case Number`, everything()) %>% \n    arrange(desc(n), `Case Number`, Name) \n  \n  # now we can see who all is in each case when there are more than one\n  \n}\n\n\n# A tibble: 199 x 8\n       n `Case Number` Name  `Charge(s)` `Associated Doc…\n   <int> <chr>         <chr> <chr>       <chr>           \n 1    11 \"\"            FAIR… Civil Diso… \"Fairlamb, Scot…\n 2    11 \"\"            JACK… Assaulting… \"Jackson, Emanu…\n 3    11 \"\"            JOHN… Obstructio… \"Johnatakis - I…\n 4    11 \"\"            MEGG… Conspiracy… \"Meggs Steele Y…\n 5    11 \"\"            MEGG… Conspiracy… \"Meggs Steele Y…\n 6    11 \"\"            NORD… Aid and Ab… \"Nordean - Comp…\n 7    11 \"\"            PETE… Knowingly … \"Peterson, Russ…\n 8    11 \"\"            SIMO… Restricted… \"Simon, Mark - …\n 9    11 \"\"            STEE… Conspiracy… \"Meggs Steele Y…\n10    11 \"\"            WILL… Knowingly … \"Williams, Rile…\n# … with 189 more rows, and 3 more variables: `Location of\n#   Arrest` <chr>, `Case Status` <chr>, `Entry Last Updated*` <chr>\n\nHow many documents do we have?\n\n\n# How many documents do we have?\ncases_table %>%\n  pull(`Associated Documents`) %>%\n  n_distinct()\n\n\n[1] 173\n\nWe want to know if there is a one-to-one or one-to-many relationship between any cases and documents. When we look at the documents column we can see that some people have the same associated documents. BROWN, Terry; CURZIO, Michael; FITCHETT, Cindy; GALLAGHER, Thomas; SWEET, Douglas all share the same “Fitchett et al - Complaint Fitchett et al - Statement of Facts Fitchett -Indictment” document. We must decide if we want to download that document just once or once for each person it’s associated with.\nWhen I last ran this code there were 169 unique document names in the column, but we really need to see how many unique document download links there are.\n\n\ndefendant_names <- cases_page %>%\n  html_nodes(\"td\") %>%\n  html_nodes(\"a\") %>%\n  html_attr('href') %>%\n  str_remove(\"/usao-dc/defendants/\")\n  \n# make a table of just download info\ndownload_links <- tibble(\n  link_names = cases_page %>% # making the link_names column \n    html_nodes(\"td\") %>% # links from html table, not from elsewhere in page\n    html_nodes(\"a\") %>%\n    html_text(),\n  link_urls = cases_page %>% # making the link_urls column\n    html_nodes(\"td\") %>% \n    html_nodes(\"a\") %>%\n    html_attr('href') %>% \n    str_c(\"https://www.justice.gov\", .) # paste on the missing domain\n) %>% # table is complete, now keep only rows with download links\n  filter(str_detect(link_urls, \"download$\")) %>% \n  unique() # no duplicates\n\n# How many unique document download links?\ndownload_links %>% pull(link_urls) %>% n_distinct()\n\n\n[1] 331\n\nHmm, so the HTML table had 169 document names but 322 unique download links. I’m guessing that a single document about multiple defendants gets a unique download link each time it’s listed for a unique defendant. Now I don’t think it will be easy to avoid downloading duplicate documents.\nWe want to keep everything organized by defendant, so we are going to loop through each row of the table and capture the download links per defendant, then while downloading we will save documents to a folder unique to each defendant even if that means the same document ends up in multiple folders.\nLet’s get the contents of each row into a list but drop the header row. Here we use html_nodes(\"tr\") from the rvest package to target table rows, which outputs an xml nodeset, but we want a regular R list, so we use map() from the purrr package to extract the table cells into a list.\n\n\n# Let's get the contents of each row into a list but drop the header row.\nrows <- cases_page %>%\n  html_nodes(\"tr\") %>% \n  map(~html_nodes(., \"td\")) %>% # gets cell content per row\n  compact() # drops empty header row\n\n\n\nNow we can iterate through each element of this list (a row from the HTML table) and do whatever we want. Let’s create a function to do what we want, then map that function over each list element.\nThe main jobs of this function are\nget the defendant’s name\nget list of download urls for their documents\ncreate a filename no longer than 45 characters for each document\ndownload docs into a folder named after defendant, also no longer than 45 characters\n\n\n# function that downloads all the docs\ndownload_defendant_docs <- function(row) {\n  \n  # get the defendant's name\n  # get list of download urls for their documents\n  # create a filename no longer than 45 characters for each document\n  # download docs into folder named after defendant\n  \n  message(\"**************************************************\")\n  \n  # who is the defendant in this row?\n  defendant <- row %>%\n    html_children() %>% \n    html_text() %>% \n    first()\n  \n  # clean the name so we can use it as a folder name\n  # and just in case any rows are missing a name, we will give them 'no_name'\n  if (nchar(defendant) > 0) {\n    defendant <- defendant %>%\n      str_replace_all(\"&\", \"and\") %>% \n      str_replace_all(\"[^a-zA-Z0-9]\", \"_\") %>% # replace all non alpha-numeric\n      str_replace_all(\"_{2,3}\", \"_\") %>% # replace extra underscores with single underscore\n      str_remove(\"[^a-zA-Z0-9]$\") %>% # remove any non-alpha-numeric at the end\n      str_to_lower()\n  } else {\n    defendant <- \"no_name\"\n  }\n  \n  # limit defendant names to 45 characters\n  if (nchar(defendant) > 45) {\n      defendant <- str_extract(defendant, \"^.{45}\")\n    }\n  \n  assert_that(\n    inherits(defendant, \"character\"),\n    length(defendant) > 0\n  )\n  \n  # there can be more than one document for a defendant so let's get the docs into a list\n  urls_to_download <- row %>%\n    html_children() %>% \n    html_nodes(\"a\") %>% \n    html_attr(\"href\") %>% \n    map(~str_c(\"https://www.justice.gov\", .)) # add missing domain on all links\n  \n  assert_that(\n    inherits(urls_to_download, \"list\"),\n    length(urls_to_download) > 0,\n    # make sure there are no NAs\n    map(urls_to_download, is.na) %>% unlist() %>% sum() == 0\n  )\n  \n  doc_names <- row %>%\n    html_children() %>% \n    html_nodes(\"li\") %>% # doc links are in html list tags\n    html_text() %>% \n    as.list() %>% \n    map(~str_remove(., \".pdf\")) %>% # a few filenames have .pdf inside the filename\n    map(~str_replace_all(., \"&\", \"and\")) %>% \n    map(~str_replace_all(., \"[^a-zA-Z0-9]\", \"_\")) %>% # replace all non alpha-numeric\n    map(~str_replace_all(., \"_{2,3}\", \"_\")) %>% # replace extra underscores with single underscore\n    map(~str_to_lower(.)) %>% \n    # limit filenames to 45 characters\n    modify_if(.p = ~nchar(.x) > 45, .f = ~str_extract(.x, \"^.{45}\")) %>% \n    map(~str_c(., \".pdf\")) # append .pdf to all filenames\n  \n  assert_that(\n    inherits(doc_names, \"list\"),\n    length(doc_names) > 0,\n    map(doc_names, is.na) %>% unlist() %>% sum() == 0\n  )\n  \n  # make sure we have the same number of urls and document names\n  assert_that(\n    length(urls_to_download) == length(doc_names)\n  )\n  \n  # a friendly message that will appear in the console each time this function runs\n  message(str_c(\"... have defendant, doc names, and links, proceeding to download for: \", defendant, \"\\n\"))\n  \n  # Confirm or create a download folder and defendant folder. The dir functions\n  # are from the fs package. The here() function is from the here package, and it helps\n  # us make file paths.\n  if (!dir_exists(here(\"downloads\"))) {\n    dir_create(here(\"downloads\"))\n  }\n  \n  if (!dir_exists(here(\"downloads\", defendant))) {\n    dir_create(here(\"downloads\", defendant))\n  }\n  \n  #---- politely download the files (will wait 5 seconds between each) ----\n\n  # function that will take a url and filename and do the downloads politley\n  get_doc <- function(url , filename) {\n    nod(session, url) %>% # nod and rip are from the polite package\n      rip(destfile = filename,\n          path = here(\"downloads\", defendant),\n          overwrite = TRUE)\n  }\n  \n  # Map our function over the two lists, but first put them in a single list\n  # that gets passed to pmap()\n  list(url = urls_to_download, filename = doc_names) %>% \n    pmap(safely(get_doc)) # wrapped in safely so it won't stop on error\n\n  message(str_c(\"...finished downloads for: \", defendant, \"\\n\\n\\n\"))\n  \n} # end function\n\n\n\nJust before running our main function I like to print a few messages to console\n\n\nmessage(str_c(\"There are \", length(rows), \" rows from wich to download documents\", \"\\n\"))\nmessage(str_c(\"There are \", nrow(download_links), \" documents to download\", \"\\n\"))\nmessage(str_c(\"There will be a 5-second pause between each document\", \"\\n\"))\nmessage(str_c(\"See https://dmi3kno.github.io/polite/reference/set_delay.html for more details\", \"\\n\\n\"))\n\n\n\nThe last thing to do is map the function over our list of rows with:\nmap(rows, download_defendant_docs)\nWow that seems like a lot of work to get to those documents! Initially it was. But stepping through all of the exploratory data analysis is important to know what to expect and how to write our functions to handle different possibilities.\nIf you run this, you will be downloading over 300 PDF files with a 5-second pause between each one. So refill that coffee!\nIf you want the code all in one piece you get it from my Github here.\n\n\n\n",
    "preview": "posts/2021-02-16-scrape-hundreds-of-pdf-documents-from-the-web-with-r-and-rvest/files.png",
    "last_modified": "2021-02-21T14:20:02-05:00",
    "input_file": {},
    "preview_width": 2030,
    "preview_height": 684
  },
  {
    "path": "posts/2021-02-15-shiny-holiday-party/",
    "title": "Shiny Holiday Party",
    "description": "Fun Shiny app to draw names at the office party",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "shiny",
      "fun"
    ],
    "contents": "\nFor an annual holiday Zoom party I was asked to draw a random team member’s name who would win a prize. So I made a Shiny app! The code is on my Github if you want it. The app is on shinyapps.io. if you want to play.\nThe Fun Part\nIn addition to picking a name, the app also searches IMDB movie plot summaries for that name, picks a random one from the results and shows it in the app without the movie title. Then everyone tries to guess the movie title. There is a “reveal” button to reveal the movie title if no one can guess.\nI removed my coworkers’ names from the app and am using a random selection from the RStudio team page. Here’s a preview, now go play!\n\n\n\n\n",
    "preview": "posts/2021-02-15-shiny-holiday-party/site_screenshot.png",
    "last_modified": "2021-02-15T22:44:32-05:00",
    "input_file": {},
    "preview_width": 1998,
    "preview_height": 1824
  },
  {
    "path": "posts/2020-05-17-twitter-bookmarks/",
    "title": "Twitter Bookmarks",
    "description": "Recently bookmarked tweets worth sharing.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-17",
    "categories": [],
    "contents": "\nRecently bookmarked tweets worth sharing\nClick an image to go there!\n\n\n\ndiv.l-screen {\n  overflow-x: scroll;\n}\n\n\n\n\n\ntweets_map_layers\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\n\n\n",
    "preview": "posts/2020-05-17-twitter-bookmarks/preview.png",
    "last_modified": "2020-05-17T14:04:29-04:00",
    "input_file": {},
    "preview_width": 3782,
    "preview_height": 1098
  },
  {
    "path": "posts/2020-05-14-twitter-by-location/",
    "title": "Twitter by Location",
    "description": "Make functions that will gather tweets by keyword in multiple geographic locations, find the most resonating and most activating tweets in each city and present them in a pretty table for reading",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-14",
    "categories": [
      "twitter",
      "web scraping",
      "api",
      "geocoding",
      "tables"
    ],
    "contents": "\n\nProblem: What are Twitter users saying about COVID-19 in different cities across the US?\n\nLoad our packages. rtweet retrieves our tweets from the Twitter API and also gets the latitude and longitude coordinates for our cities by sending our city names through the Google Maps API. We will use the reactable package to make beautiful custom tables to read our top tweets in each city.\n\n\n# do we need to get more tweets?\nneed_online_tweets <- FALSE\nif(need_online_tweets == FALSE) {\n  tweets <- readRDS(fs::dir_ls(here::here(\"_posts\",\n                               \"2020-05-14-twitter-by-location\"),\n                               regexp = \".*covid_tweets.*\"))\n} else { # download more tweets:\n\n#---- some helper functions for later\n  \n# search twitter\nget_tweets <- function(locs, query) {\n  \n  search_tweets(\n    q = query,\n    geocode = locs,\n    n = 2000,\n    include_rts = FALSE\n  )\n  \n}\n\n# get coordinates for the places we want to search\nget_us_coords <- function(place, mile_radius) {\n  \n  # function output should be a character vetcor \n  # that looks like: \"40.397408,-102.054770,130mi\"\n  # with no spaces, which is the format required by\n  # the twitter api\n  \n  if(!inherits(place, \"character\")) {\n    stop (\"place must be a quoted character string\")\n  }\n  \n  if(!inherits(mile_radius, \"character\")) {\n    stop (\"mile_radius must be a quoted character string\")\n  }\n\n  # requires google maps api key\n  rtweet::lookup_coords(\n    address = place,\n    components = \"country:US\",\n    apikey = Sys.getenv(\"GOOGLE_MAPS_KEY\")\n    ) %>%\n    .[[\"point\"]] %>%\n    paste0(., collapse = \",\") %>%\n    paste0(., \",\", mile_radius, \"mi\")\n  \n}\n\n#--- now to using those functions to get the tweets we want\n\n# list of places to search for tweets\nmy_places <- list(\n  \"New York, New York\",\n  \"Atlanta, Georgia\",\n  \"Chicago, Illinois\",\n  \"Dallas, Texas\",\n  \"Denver, Colorado\",\n  \"Phoenix, Arizona\",\n  \"Seattle, Washington\",\n  \"San Francisco, California\"\n)\n\n# map our get_us_coords() function over our list of places to get a list of\n# lat long points with bundled radius for each place\nlocs <- purrr::map(\n  my_places,\n  get_us_coords,\n  mile_radius = \"55\"\n)\n\ntime_of_search <- Sys.time()\n\n# map our get_tweets() function over our list of places\n# then combine results and remove duplicate tweets\ndat_list <- purrr::map(\n  .x = locs,\n  .f = get_tweets,\n  query = \"COVID\"\n) %>% # name each list element with its location name, to use in bind_rows below\n  set_names(my_places)\n\ndat <- bind_rows(\n  dat_list,\n  .id = \"city_searched\" # this column will be filled with the list element names\n) %>% unique()\n\n\ntweets <- dat %>% \n  select( # reorder columns\n    city_searched,\n    location,\n    created_at,\n    screen_name,\n    name,\n    text,\n    hashtags,\n    favorite_count,\n    retweet_count,\n    quote_count,\n    reply_count,\n    status_id,\n    everything()\n  ) %>% # the geo_coords column is a list column of lat-lng vectors, so unnest_wider\n  mutate(geo_coords = map(geo_coords, ~set_names(., c(\"lat\", \"lng\")))) %>% \n  unnest_wider(geo_coords) %>% \n  arrange(\n    desc(created_at)\n  )\n\n#---- write the tweets to disk\n\n# format time for use in file name\nmy_time <- str_replace_all(time_of_search, \" \", \"_\")\nmy_time <- str_replace_all(my_time, \":\", \".\")\nmy_file_name <- paste0(\"covid_tweets_\", my_time)\n\n# write tweets to disk\nsaveRDS(tweets, here::here(my_file_name))\n#rio::export(tweets, here::here(my_file_name))\n\n}\n\n\n\n\nGroup by city, arrange the favorite_count column in descending order, and limit the columns we display.\n\n\n# favorited\nmost_resonating <- tweets %>% \n  select(city_searched,\n         text,\n         favorite_count,\n         hashtags,\n         screen_name,\n         created_at,\n         retweet_count) %>% \n  group_by(city_searched) %>%\n  arrange(desc(favorite_count)) %>% \n  slice(1:10)\n  \n\n# retweeted\nmost_activating <- tweets %>% \n  select(city_searched,\n         text,\n         retweet_count,\n         hashtags,\n         screen_name,\n         created_at,\n         favorite_count\n         ) %>%\n  group_by(city_searched) %>%\n  arrange(desc(retweet_count)) %>% \n  slice(1:10)\n\n\n\n\nSet some basic CSS for our table class and the class of our text column.\n\n\n.tweet-tbl {\n  font-size: .6em;\n}\n\n.text-col {\n  font-weight: 600;\n  color: #e96384;\n}\n\n\n.tweet-tbl {\n  font-size: .6em;\n}\n\n.text-col {\n  font-weight: 600;\n  color: #e96384;\n}\n\n\n\nThe table showing the top 10 most favorited tweets in each city. Click a city to expand its rows. Scroll right to see more columns. First up are the tweets that resonate the most, i.e., the tweets most favorited.\n\n\n# a color palette function to shade table cells based on a value\nmy_pal <- function(x) rgb(colorRamp(c(\"#f0f0f5\", \"#a3a3c2\"))(x), maxColorValue = 250)\n\nresonate_table <- reactable(\n  most_resonating,\n  groupBy = \"city_searched\",\n  filterable = TRUE,\n  class = \"tweet-tbl\",\n  height = 600,\n  columns = list(\n    text = colDef(\n      class = \"text-col\", # see CSS chunk for styling\n      minWidth = 400\n    ),\n    favorite_count = colDef(style = function(value) {\n      normalized <- (value - min(most_resonating$favorite_count)) / \n        (max(most_resonating$favorite_count) - min(most_resonating$favorite_count))\n      color <- my_pal(normalized)\n      list(background = color)\n      }\n    ),\n    hashtags = colDef(style = list(borderLeft = \"2px solid #8585ad\"))\n  )\n)\n\n\n\nTop 10 most resonating tweets in each city\n\n\n{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"city_searched\":[\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\"],\"text\":[\"For those protesting social distancing (doubt they would read this) - a small group of my friends got together for lunch 10 days ago: 1 is on a vent, another admitted to a regular floor bed, 5 others are COVID + at home. You can be asymptomatic and have COVID19\",\"This is why governors can't just have unchecked power to order whatever they want. Because sometimes they make mistakes like this. #AndrewCuomo #Coronavirus #NewYork https://t.co/uUgupW6woU\",\"CDC tracks 12 different forecasting models of possible #COVID19 deaths in the US. As of May 11, all forecast an increase in deaths in the coming weeks and a cumulative total exceeding 100,000 by June 1. See national &amp; state forecasts: https://t.co/PI1AtLCCmt https://t.co/iylBnom5U0\",\"\\\"A group of nurses at Methodist Healthcare System say they'll do whatever they can to help a San Antonio woman - and BTS fan - who is fighting COVID-19\\\"\\n\\n\\\"Some of the nurses even posted a TikTok video to get the band's attention\\\"\\n@BTS_twt #EncourageEsther \\nhttps://t.co/267XAeXiep\",\"CDC tracks 12 different forecasting models of possible #COVID19 deaths in the US. As of May 11, all forecast an increase in deaths in the coming weeks and a cumulative total exceeding 100,000 by June 1. See national &amp; state forecasts: https://t.co/Ft6cgmaMPX. https://t.co/jX4fQK5Q1V\",\"I couldn’t get the tooth tho. She was sleeping right smack on top of her pillow and I couldn’t get my hand in. I slid the money under the pillow and in the morning when she asks me why the tooth fairy didn’t take her tooth I’m gonna say, “Covid-19, she don’t know you like that.”\",\"These should be forbidden post COVID! Clearly whoever designed it did not live through a pandemic 🙄 https://t.co/AwQ1rya9Y3\",\"Abbott is changing the instructions of their rapid coronavirus test: they're now telling anyone with signs of COVID-19 who tests negative with the Abbott test that they should confirm this result by taking a different test than the Abbott test\\n\\nhttps://t.co/LOCn4ww1Zp\",\"Red Cross and Red Crescent teams work on the front lines to support communities responding to the impacts of COVID-19.\\n\\nThe #CocaColaFoundation has awarded nearly $14M in grants in 60+ countries @ICRC @ifrc @redcross. Learn more: https://t.co/Olhop9ICzu https://t.co/cEGpFt7MXO\",\"Dr. Stanley helps us remember that God is able to restore our souls even if we are experiencing grief and loss during the trying times that COVID-19 has caused. https://t.co/uVmwB4zXL7 https://t.co/L5hOuSN4c7\",\"Thank you @gherbo for your leadership, resilience, and desire to bring change, resources, and opportunities to our people 💪🏾 https://t.co/93pol2IXRE https://t.co/pJRqMQe0Sg\",\"@AlexBerenson @jack you've been claiming for months that your wife is a physician who is treating COVID patients. She is a forensic psychiatrist.\",\"If Alex Berenson's wife, a forensic psychiatrist, is treating COVID patients, as Alex loves to claim when pressed on his bullshit, someone should probably tell the state medical board! https://t.co/JZZMIVlduT\",\"@wsbtv COVID-19, Murder Hornets &amp; Giant Hogweed coming together to end humanity https://t.co/jzhMK7qzD4\",\"Do you all think Florida is lying about numbers? \\n\\nFlorida and Georgia both are lax on everything and I feel like they just aren't reflecting the numbers of COVID anymore!\",\"Danny Beers has been collecting food for LGBTQ seniors in affordable housing since the COVID-19 crisis began. He worked with @CenteronHalsted to create a pop-up pantry to enable residents to get food without making a trip to the store. https://t.co/02fyVgQH89\",\"@AlexBerenson @j_kittycat Cool. In what capacity is a forensic psychiatrist \\\"treating patients for COVID\\\"\",\"@AlexBerenson Cool. In what capacity is a forensic psychiatrist \\\"treating patients for COVID\\\"\",\"Anyway, instead of listening to millionaire Matt Y’s vErY SeRiOuS PuNdiTmAn declaration that journalists are somehow too harsh when reporting on Trump’s insane comments, I wrote something that proves that the exact opposite is true https://t.co/NZTXeUsafk\",\"Testing keeps ramping up in the US.  A month ago it was roughly 100K tests per day, now it’s 300K per day.  And, yet, the trend in new cases of COVID are on a downward trend.  #OpenUpAmerica https://t.co/AZ5lgHTbaJ\",\"BRAZIL REPORTS RECORD 13,944 COVID-19 CASES IN 24 HOURS\\n\\n(It’s 69 degrees with 87% humidity in Sao Paolo now)\\n\\n@business\",\"Did y’all hear the piece on CNN that revealed if you’re in a restaurant with one who is asymptomatic &amp; they talk for 25 minutes they release 1000s of covid particles in the air that remain contagious for 12 minutes? Black news said that weeks ago but it was ignored. Stay safe\",\"Former neurosurgeon says masks are ineffective against COVID-19 and can cause health problems https://t.co/J7oyWEdWPj\",\"If you believe the number of COVID-19 deaths is accurate, you probably also believe the Left will be able to count mail-in-voting ballots correctly.\\n\\nAnd you're actually wrong on both counts.\",\"Joe Rogan: I might move to Texas 'if California continues to be this restrictive' with 'silly' COVID-19 shutdown https://t.co/JzebzCoPJa\",\"Dallas County could see summer surge in COVID-19 cases. According to UTSW scientists what happens is up to you. 69% overall compliance with #SaferAtHome and we knock cases down to only a few a day. 60% and we ballon to 800 a day by July. #FlattenTheCurve https://t.co/I0BCvC3RZH\",\"Joe Rogan: I might move to Texas 'if California continues to be this restrictive' with 'silly' COVID-19 shutdown https://t.co/tqihURV0gA\",\"To all of the frontline workers during COVID-19, we salute you. Thank you for everything you’ve done and are doing, and this weekend, we race for you.\\n\\n#TheRealHeroes // #NASCARisBack https://t.co/75Y1q3S006\",\"Dan Crenshaw sets the record straight on Trump's COVID response: 'I just want people to understand the truth' https://t.co/dUJnjIMUL9\",\"China has arrested nearly 500 people for speaking out about COVID-19, report says https://t.co/dIje3OTROE\",\"@elonmusk @TheZenCorner @stoolpresidente It's even worse than that Elon...\\n\\nMan found dead on the sidewalk with a .55 blood alcohol level and the Colorado Dept of Health made the Country Coroner categorize the death as Corona. This whole thing is a fraud!\\n\\nhttps://t.co/bMJ976DaMv\",\"NEW: A woman jailed in Colorado for what investigators describe as armed child kidnapping plot by QAnon believers, is asking to be released due to the COVID-19 pandemic, which QAnon conspiracy theorists have suggested is a hoax. #9NEWS #COVID19Colorado\",\"I am being induced next week and just found out that I will be getting a pre-screening COVID test tomorrow 😬 prayers please that it comes back negative and that the experience isn't horrible\",\"This is promising! Some states are waking up to the fact that lying about the cause of death on official Death Certificates is not a good idea.\\nIllinois decided to remove inaccurate numbers from the COVID19 death toll, but did not go for full correction:\\nhttps://t.co/26UIWim6g3\",\"Hey MLB players, wanna sound tone def and alienate millions of Americans who lost their jobs from Covid-19?  Just do what Tampa Bay Rays Blake Snell is doing. Snell signed a 5 year $50 million contract in 2019. https://t.co/8wyC5FPZXg\",\"QAnon has grafted into conspiracy theories about COVID-19 after beginning with the claim that President Trump is fighting a secret war against a global pedophilia ring of Democrats who drink the blood of children. Which made for an awkward toss to our weather segment... https://t.co/awCrb4agmc\",\"The #AtlasV #USSF7 launch will honor all front-line workers and #COVID19 first responders while paying tribute to those affected by the pandemic #AmericaStrong\\n\\nhttps://t.co/7UFh0IFK34 https://t.co/GDkb55JLjb\",\"What killed Sebastian Yellow? Coroner says the man drank himself to death with .550 blood alcohol reading-'ethanol toxicity' reads death certificate. But @CDPHE has categorized it as a #COVID19 death,  raising many questions. Our @CBSDenver report:\\nhttps://t.co/w5dHyT24wk https://t.co/viEClhYeBX\",\"Vitamin D deficiency is scientifically proven to be associated with a lower immune system and a higher risk for viral diseases. So let's keep everyone indoors, away from sunlight and Vitamin D production, to prevent covid!\",\"NEW: A Republican state representative is calling for criminal charges against Colorado's top public health official, accusing her of falsifying records to inflate COVID-19 death totals. #9NEWS #COVID19Colorado\",\"Inbox: Sen. Kamala Harris has led her colleagues in a letter to FEMA Administrator Gaynor and HHS Secretary Azar demanding an accurate death count and consistent statistics during the COVID-19 pandemic, after the Trump admin suggested the death toll might be lower than reported.\",\"El Paso health director announces \\\"dramatic spike in positive cases\\\"\\nhttps://t.co/K1ALVrCs0D\",\"I will not stop fighting for our frontline workers just like they’re fighting for us\\n\\nI’ve been fighting for essential workers to get premium pay from a Heroes Fund\\n\\nAnd I’m proud $200B for this in the @HouseDemocrats’ bill\\n\\nSen. McConnell—We must act NOW\\n\\nhttps://t.co/TuDUHl0WZh\",\"In episode 37 of Common Sense, we sit down with Dr. Steven Greer who provides a valuable perspective in analyzing our COVID-19 response.\\n\\nAs Doctor Greer states in today's episode, \\\"the goalpost keeps moving.\\\"\\n\\nhttps://t.co/MSo2ugUT1U https://t.co/1Ku6OEZWi1\",\"India exports 50 million hydroxychloroquine tablets to U.S. for COVID-19 fight: source | Article [AMP] | Reuters https://t.co/LvTugornLr\",\"As @HouseJudiciary Chair, I've made protecting the inmate population from COVID-19 a priority. I’m proud that the HEROES Act includes my bill to provide funds to keep our federal prisons safe and help state and local facilities stop the spread of COVID-19.\\nhttps://t.co/dWb53OqXLm\",\"Today @GeorgiaEMA received over 9,000 personal-sized bottles of hand sanitizer from our friends at @AnheuserBusch. Thankful for our corporate partners as we continue to fight COVID-19!\",\"@middlebrooks According to Tom Verducci's article MLB is not using existing COVID-19 tests, they are turning the lab they use for PED testing into a COVID-19 testing lab, so they will actually be adding a net gain to public testing (testing players, support staff, etc) not taking away from it.\",\"A majority of Taiwanese adults hold negative views of mainland China, and support closer economic AND political ties with the United States.\\n\\nThe Pew survey was conducted several months before COVID-19 outbreak and the sweeping re-election of DPP's Tsai Ing-wen in Jan 2020. https://t.co/wQJ3FZA1iB\",\"Deporting COVID+ patients to countries with medical systems less able to deal with this complicated disease is IRRESPONSIBLE.  We can deal with it, and we should.  EVERYONE deserves high quality care and no person deserves to be put unnecessarily at risk.\",\"TRUMP says “FU Fauci” business &amp; schools will reopen. Will Fauci, who works for BIG PHARMA &amp; VACCINE pushers, RETALIATE with new  FAKE PANDEMIC? Is Fauci Dr Death of the economy?  No body wants to die. If you are afraid of Covid stay home, wear a mask &amp; LET THE REST OF US LIVE.\",\"Prayers to Art Howe, one of the nicest men who ever wore a baseball uniform, who's in intensive care in Houston with COVID-19.\",\"If you idiots fall for this clear distraction of the fact that trump let 80k people die, 33 million go unemployed, and have the worst response to Covid than any other developed country in the world, I’m gonna scream https://t.co/cRYPfKq1mx\",\"From LA &amp; PHX to Asheville, NC &amp; WV, grassroots progressives did physically-distanced protests today to call for a #PeoplesBailoutBloc in the House to stop a vote on the Heroes Act unless paycheck guarantee, $2K monthly checks, &amp; free COVID treatment for all are added to bill. https://t.co/O3fI0GfDZH\",\"COVID-19 originated in China. Multiple reports have stated that if China acted sooner and alerted the world, 95% of this could have been avoided.\\n\\nThe fact China wants to punish countries for accurately saying that shows they aren't the world's allies. https://t.co/A38kh0cRTA\",\"JUST IN: The number of COVID-19 cases in Arizona has jumped to 12,674. According to the latest numbers from ADHS, there have been 624 deaths due to the virus statewide.\\nLatest: https://t.co/S6OO1tEW0B\\n#ABC15 #Coronavirus https://t.co/TGO0MvLh3y\",\"On a scale of 1-1984, how are you feeling right now? Muzzles (masks, that is), arrows &amp; boxes on the floor telling you where you can &amp; can’t go, “show me your papers,” prove you’re essential, neighborly snitching, “wiretapping” of political enemies...#TruthNotFiction #COVID https://t.co/R8XmVcSN09\",\"It's probably just as well that Donald Trump hasn't tried to reach out to those grieving over loved ones lost to Covid 19. The man does not do empathy well.\",\"Another example of Democrats shooting themselves in the foot.\\n\\nIf they want to keep the House, they need their moderates. Yet this bloated relief package that will never pass is forcing them to vote for a far-left wish list.\\n\\nGreat for @GOP in 2020! https://t.co/QOHeXfnM8Y\",\"@aterkel \\\"There’s really no other way to say this: When it comes to his response to the spread of COVID-19 in the United States, the combination of President Donald Trump’s arrogance and sheer incompetence is killing people.\\\"\\n\\nhttps://t.co/PTwys8zGXa\",\"SCOOP: I obtained a whistleblower complaint about Stanford's COVID-19 study.\\n\\nTurns out JetBlue's founder, a critic of the economic shutdowns, helped fund it.\\n\\nAnd John Ioannidis and others allegedly ignored internal scientists' concerns about the test. \\n\\nhttps://t.co/dExNhZTVX0\",\"Important, please share!\\n\\nAnyone going in to work in San Francisco, or anyone who lives in SF and has one or more symptoms, can get a FREE COVID-19 test.\\n\\nNo insurance required, and you get results in 1-3 days.\\n\\nSign up today and #StopTheSpreadSF at https://t.co/VMsxpuvzTL https://t.co/z1VkVCme9U\",\"In the 30 yrs after WWII, working-class wages rose 91%.\\n\\nIn the last 40 years? Working-class wages were stagnant.\\n\\nNow Americans don’t have the savings they need to make it through COVID. Folks don’t just need jobs: they need a raise.\\n\\nLet's send $2k/month to start.\",\"🤪🤪🤪🤪🤪🤪🤪🤪🤪🤪🤪🤪Trump superfan shows symptoms of COVID-19 after getting kicked out of grocery for refusing to wear mask https://t.co/50P4NujyoR\",\"Scammers are everywhere, even when you're job hunting.\\n\\nHere are 5 scam indicators you should be aware of if you're currently on the hunt, courtesy of @Forbes. 🕵️‍♀️🕵️‍♂️\\n https://t.co/CFgFQdGr8z\",\"Damion hopped on the Hang Time podcast with @SekouSmithNBA to talk about his basketball career, family members on the frontlines during COVID-19, and overcoming obstacles.\\n\\n» https://t.co/eoYhPhYVC2 https://t.co/KSyYOzMWS0\",\"And that plenty of doctors themselves are racist and are treating Black people and other people of color with COVID-19 differently than they are white people with COVID-19 https://t.co/MtS4T8Uv0E\",\"Come tell me about Elon Musk. Here is my latest: https://t.co/6WVbKIQ37v https://t.co/xREOy8eqLx\",\"If we let police use drone surveillance with face recognition as a public health measure--we’ll likely see them flying over protests next year. https://t.co/dJ3L6U9Pne\",\"Saudi Arabia should #FreeLoujain immediately. \\n \\nNo prisoner of conscience should spend a day in jail. Particularly with the threat of COVID-19, pushing for gender equality should never be a death sentence. https://t.co/EQpKVtVIcy\",\"Nicaragua en medio de 4 pandemias: el covid-19, la pobreza, la ignorancia y la corrupción 🥺\",\"Wow. ⁦@RandPaul⁩ Chief Strategist doesn’t understand that having #COVID and being asymptomatic does not mean you will NOT develop serious COVID symptoms and/or die. https://t.co/xdqpMtuAaQ\",\"Gov Inslee: \\\"We should not be intimidated when people say, ‘Oh, you can’t use this COVID crisis, you know, to peddle a solution to climate change.’\\\"\\n\\nInslee doesn't even hide it: he will happily use the coronavirus for his radical, economy killing environmental agenda. https://t.co/yQfbwVh9Rh\",\"A resident of rural Snohomish County, \\\"Jean\\\" had COVID-like symptoms in late Dec 2019 and has subsequently tested positive in a serological assay. This may have been COVID-19 infection, but it's not certain (or even likely). 1/7\\nhttps://t.co/7GlZyAmwsu\",\"For the first time in over two months, King County recorded no new deaths from COVID-19 in its daily update.\\n\\nhttps://t.co/QEnIGRBzhD\",\"Thus, it's possible that Jean had COVID in December, but it's much more likely to have been an asymptomatic infection in the intervening months. 7/7\",\"My parents live in a town with more cows then people. No one wears masks. People there think Covid is fake. My parents have isolated as much as they can, only going out for groceries. Today my mom got sick and her chest x-rays showed pneumonia symptoms. Waiting on test. Fuck this https://t.co/zKBbUv2zGx\",\"WATCH: Update on State Response to COVID-19 https://t.co/KS7HSYxION\",\"UPDATE: A glitch?  A?  What? Just a CDC announcement to get a flu shot at 11:26pm. Not COVID-19 references. No mention of pandemic. Nope. Just \\\"get a flu shot and maybe wash your hands.\\\" WHAT IS THIS.\",\"Given these probabilities, we can compare hypotheses under equal priors. We get a 9% probability of scenario 1 (false positive in serological assay), 89% probability of scenario 2 (asymptomatic COVID-19 infection) and 2% probability of scenario 3 (COVID-19 infection in Dec). 6/7\"],\"favorite_count\":[1924,635,589,544,361,235,234,216,184,174,859,428,305,215,183,179,140,139,134,132,586,578,531,362,294,282,267,160,157,150,535,402,351,298,285,259,253,208,166,154,1032,770,520,446,383,206,138,134,134,113,3630,2297,177,157,130,125,125,111,107,95,1603,309,286,192,187,137,125,115,110,106,1004,811,495,482,311,275,260,260,178,154],\"hashtags\":[[null],[\"AndrewCuomo\",\"Coronavirus\",\"NewYork\"],[\"COVID19\"],[\"EncourageEsther\"],[\"COVID19\"],[null],[null],[null],[\"CocaColaFoundation\"],[null],[null],[null],[null],[null],[null],[null],[null],[null],[null],[\"OpenUpAmerica\"],[null],[null],[null],[null],[null],[\"SaferAtHome\",\"FlattenTheCurve\"],[null],[\"TheRealHeroes\",\"NASCARisBack\"],[null],[null],[null],[\"9NEWS\",\"COVID19Colorado\"],[null],[null],[null],[null],[\"AtlasV\",\"USSF7\",\"COVID19\",\"AmericaStrong\"],[\"COVID19\"],[null],[\"9NEWS\",\"COVID19Colorado\"],[null],[null],[null],[null],[null],[null],[null],[null],[null],[null],[null],[null],[null],[\"PeoplesBailoutBloc\"],[null],[\"ABC15\",\"Coronavirus\"],[\"TruthNotFiction\",\"COVID\"],[null],[null],[null],[null],[\"StopTheSpreadSF\"],[null],[null],[null],[null],[null],[null],[null],[\"FreeLoujain\"],[null],[\"COVID\"],[null],[null],[null],[null],[null],[null],[null],[null]],\"screen_name\":[\"MSharifpourMD\",\"THEHermanCain\",\"CDCDirector\",\"BTSBEINGBTSYT\",\"CDCgov\",\"KyleJamesHoward\",\"MSharifpourMD\",\"lookner\",\"CocaColaCo\",\"InTouchMin\",\"workwthecoach\",\"lib_crusher\",\"lib_crusher\",\"WhiteHairedMC\",\"SpeakUpNoworNev\",\"chicagosmayor\",\"lib_crusher\",\"lib_crusher\",\"ParkerMolloy\",\"wesbury\",\"DiMartinoBooth\",\"fhunscripted\",\"theblaze\",\"TheDamaniFelder\",\"theblaze\",\"JudgeClayJ\",\"theblaze\",\"Chris_Buescher\",\"theblaze\",\"theblaze\",\"ReidarSundy\",\"KyleClark\",\"maryrezacfarrow\",\"NikolovScience\",\"byesline\",\"KyleClark\",\"ulalaunch\",\"Briancbs4\",\"LivePDDave1\",\"KyleClark\",\"kylegriffin1\",\"maddow\",\"SenSchumer\",\"RudyGiuliani\",\"intheMatrixxx\",\"RepJerryNadler\",\"GovKemp\",\"HeidiWatney\",\"MsMelChen\",\"cameronks\",\"theRealKiyosaki\",\"BNightengale\",\"OGBobbyGabriel\",\"kai_newkirk\",\"DanKEberhart\",\"abc15\",\"kelliwardaz\",\"Brimshack\",\"DanKEberhart\",\"JG_Vanguardia\",\"stephaniemlee\",\"LondonBreed\",\"RepRoKhanna\",\"AngryBlackLady\",\"Chime\",\"warriors\",\"AngryBlackLady\",\"karaswisher\",\"EFF\",\"RepRoKhanna\",\"Fernanda__Mejia\",\"NaveedAJamali\",\"jasonrantz\",\"trvrb\",\"komonews\",\"trvrb\",\"BryanGo\",\"GovInslee\",\"nathanbarnes\",\"trvrb\"],\"created_at\":[\"2020-05-15T11:01:23\",\"2020-05-15T11:14:03\",\"2020-05-15T21:20:03\",\"2020-05-15T20:22:17\",\"2020-05-15T14:35:02\",\"2020-05-15T05:48:09\",\"2020-05-14T23:14:58\",\"2020-05-15T02:57:56\",\"2020-05-15T14:50:31\",\"2020-05-15T19:46:02\",\"2020-05-15T19:41:18\",\"2020-05-15T14:52:33\",\"2020-05-15T15:07:25\",\"2020-05-15T18:50:33\",\"2020-05-15T15:51:10\",\"2020-05-15T17:16:39\",\"2020-05-15T15:00:47\",\"2020-05-15T15:06:13\",\"2020-05-15T20:36:13\",\"2020-05-15T17:14:58\",\"2020-05-14T22:39:51\",\"2020-05-15T02:00:59\",\"2020-05-15T20:00:07\",\"2020-05-15T12:42:45\",\"2020-05-15T00:00:00\",\"2020-05-15T17:23:07\",\"2020-05-15T05:30:07\",\"2020-05-14T23:06:41\",\"2020-05-15T13:00:09\",\"2020-05-14T22:00:25\",\"2020-05-15T03:34:34\",\"2020-05-15T04:10:45\",\"2020-05-15T02:08:54\",\"2020-05-15T03:52:57\",\"2020-05-14T16:51:35\",\"2020-05-15T04:16:04\",\"2020-05-14T20:14:47\",\"2020-05-15T00:30:48\",\"2020-05-14T22:07:17\",\"2020-05-15T05:03:28\",\"2020-05-15T21:15:38\",\"2020-05-15T21:41:03\",\"2020-05-15T21:19:09\",\"2020-05-15T20:30:04\",\"2020-05-15T21:37:08\",\"2020-05-15T20:36:18\",\"2020-05-15T20:50:46\",\"2020-05-15T21:25:10\",\"2020-05-15T20:48:07\",\"2020-05-15T20:29:32\",\"2020-05-14T15:41:31\",\"2020-05-14T23:22:52\",\"2020-05-14T15:29:03\",\"2020-05-15T04:03:25\",\"2020-05-14T17:31:00\",\"2020-05-14T16:03:19\",\"2020-05-14T13:22:34\",\"2020-05-14T15:24:31\",\"2020-05-15T14:49:21\",\"2020-05-14T21:12:55\",\"2020-05-15T21:15:27\",\"2020-05-15T18:33:29\",\"2020-05-15T16:07:00\",\"2020-05-15T15:39:18\",\"2020-05-15T14:58:37\",\"2020-05-15T19:01:00\",\"2020-05-15T14:53:04\",\"2020-05-15T17:01:51\",\"2020-05-15T17:45:00\",\"2020-05-15T17:38:08\",\"2020-05-15T00:21:11\",\"2020-05-15T01:40:59\",\"2020-05-15T18:24:51\",\"2020-05-15T19:24:03\",\"2020-05-14T23:32:58\",\"2020-05-15T19:24:04\",\"2020-05-15T00:09:03\",\"2020-05-14T22:35:27\",\"2020-05-15T06:29:36\",\"2020-05-15T19:24:04\"],\"retweet_count\":[776,281,707,291,281,3,25,141,37,37,242,7,17,51,29,22,1,0,25,35,166,213,282,85,70,139,43,28,56,93,111,97,9,175,19,50,48,177,85,42,351,455,149,216,216,73,16,17,24,29,822,399,69,65,42,81,71,4,29,15,983,233,70,28,48,10,22,27,71,33,410,239,230,204,72,24,31,78,4,15]},\"columns\":[{\"accessor\":\"city_searched\",\"name\":\"city_searched\",\"type\":\"character\"},{\"accessor\":\"text\",\"name\":\"text\",\"type\":\"character\",\"minWidth\":400,\"className\":\"text-col\"},{\"accessor\":\"favorite_count\",\"name\":\"favorite_count\",\"type\":\"numeric\",\"style\":[{\"background\":\"#CCCCDF\"},{\"background\":\"#E9E9F2\"},{\"background\":\"#EAEAF3\"},{\"background\":\"#EBEBF3\"},{\"background\":\"#EFEFF6\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#E4E4EF\"},{\"background\":\"#EDEDF5\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#EAEAF3\"},{\"background\":\"#EAEAF3\"},{\"background\":\"#EBEBF3\"},{\"background\":\"#EFEFF6\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#EBEBF3\"},{\"background\":\"#EEEEF5\"},{\"background\":\"#EFEFF6\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F8\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#E0E0EC\"},{\"background\":\"#E6E6F0\"},{\"background\":\"#EBEBF4\"},{\"background\":\"#EDEDF5\"},{\"background\":\"#EEEEF6\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4FA\"},{\"background\":\"#A6A6C6\"},{\"background\":\"#C4C4D9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4FA\"},{\"background\":\"#F5F5FA\"},{\"background\":\"#F5F5FA\"},{\"background\":\"#D3D3E4\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F3F3F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4FA\"},{\"background\":\"#F4F4FA\"},{\"background\":\"#F5F5FA\"},{\"background\":\"#E1E1ED\"},{\"background\":\"#E5E5EF\"},{\"background\":\"#ECECF4\"},{\"background\":\"#ECECF4\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"}]},{\"accessor\":\"hashtags\",\"name\":\"hashtags\",\"type\":\"list\",\"style\":{\"borderLeft\":\"2px solid #8585ad\"}},{\"accessor\":\"screen_name\",\"name\":\"screen_name\",\"type\":\"character\"},{\"accessor\":\"created_at\",\"name\":\"created_at\",\"type\":\"Date\"},{\"accessor\":\"retweet_count\",\"name\":\"retweet_count\",\"type\":\"numeric\"}],\"pivotBy\":[\"city_searched\"],\"filterable\":true,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"className\":\"tweet-tbl\",\"height\":\"600px\",\"dataKey\":\"ee8b01bcfaf46b286dac45921ec6fe50\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\n\nNext up are the tweets that activate the most, i.e., the tweets most retweeted.\n\n\nactivate_table <- reactable(\n  most_activating,\n  groupBy = \"city_searched\",\n  filterable = TRUE,\n  class = \"tweet-tbl\",\n  height = 600,\n  columns = list(\n    text = colDef(\n      class = \"text-col\", # see CSS chunk for styling\n      minWidth = 400\n    ),\n    retweet_count = colDef(style = function(value) {\n      normalized <- (value - min(most_activating$retweet_count)) / \n        (max(most_activating$retweet_count) - min(most_activating$retweet_count))\n      color <- my_pal(normalized)\n      list(background = color)\n      }\n    ),\n    hashtags = colDef(style = list(borderLeft = \"2px solid #8585ad\"))\n  )\n)\n\n\n\nTop 10 most activating tweets in each city\n\n\n{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"city_searched\":[\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Atlanta, Georgia\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Chicago, Illinois\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Dallas, Texas\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"Denver, Colorado\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"New York, New York\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"Phoenix, Arizona\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"San Francisco, California\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\",\"Seattle, Washington\"],\"text\":[\"For those protesting social distancing (doubt they would read this) - a small group of my friends got together for lunch 10 days ago: 1 is on a vent, another admitted to a regular floor bed, 5 others are COVID + at home. You can be asymptomatic and have COVID19\",\"CDC tracks 12 different forecasting models of possible #COVID19 deaths in the US. As of May 11, all forecast an increase in deaths in the coming weeks and a cumulative total exceeding 100,000 by June 1. See national &amp; state forecasts: https://t.co/PI1AtLCCmt https://t.co/iylBnom5U0\",\"\\\"A group of nurses at Methodist Healthcare System say they'll do whatever they can to help a San Antonio woman - and BTS fan - who is fighting COVID-19\\\"\\n\\n\\\"Some of the nurses even posted a TikTok video to get the band's attention\\\"\\n@BTS_twt #EncourageEsther \\nhttps://t.co/267XAeXiep\",\"CDC tracks 12 different forecasting models of possible #COVID19 deaths in the US. As of May 11, all forecast an increase in deaths in the coming weeks and a cumulative total exceeding 100,000 by June 1. See national &amp; state forecasts: https://t.co/Ft6cgmaMPX. https://t.co/jX4fQK5Q1V\",\"This is why governors can't just have unchecked power to order whatever they want. Because sometimes they make mistakes like this. #AndrewCuomo #Coronavirus #NewYork https://t.co/uUgupW6woU\",\"Abbott is changing the instructions of their rapid coronavirus test: they're now telling anyone with signs of COVID-19 who tests negative with the Abbott test that they should confirm this result by taking a different test than the Abbott test\\n\\nhttps://t.co/LOCn4ww1Zp\",\"If you have signs of a heart attack or stroke while staying at home during #COVID19, call 9-1-1 right away. Emergency responders and emergency departments have plans in place to help protect you from COVID-19. \\n\\nLearn more: https://t.co/4i0e7Cw3m1. https://t.co/ZqLIrVCHXI\",\"Criminals often take advantage of world events such as #COVID19. If you think you’re a victim of a scam or attempted fraud involving COVID-19, contact the National Center for Disaster Fraud hotline by phone at 1-866-720-5721 or email at disaster@leo.gov. https://t.co/oJWSeGXMYm\",\"It’s a lot of folks with covid who have no symptoms. The problem is they’re passing the virus to people who will lose their lives. That’s why this isn’t just about you and your desires. This is about community and how we care for each other.\",\"Good morning! Interested in how #COVID19 is affecting journal submissions @isanet journals? Curious about gender gaps? @drkristawiegand (@ISQ_Jrnl), @debbielisle (@INTPOLITSOCIO), @JamesMS24 (@ISP_Journal), and I (@IntlStudiesRev) have a blog post for you! https://t.co/N8YR3wBgso\",\"Thank you @gherbo for your leadership, resilience, and desire to bring change, resources, and opportunities to our people 💪🏾 https://t.co/93pol2IXRE https://t.co/pJRqMQe0Sg\",\"Wisconsin COVID-19 cases jump by 410 today, biggest one-day increase since May 1. Racine County continuing to climb, with 920 cases and rate per 100,000 residents now third-highest in state. Kenosha with 780 cases.\",\"@wsbtv COVID-19, Murder Hornets &amp; Giant Hogweed coming together to end humanity https://t.co/jzhMK7qzD4\",\"COVID-19 has killed more incarcerated people since March 26th in the US, than the death penalty has in the last 10 complete years.\",\"Nearly 90,000 Americans have died from COVID-19, but please, show us your Star Trek flag. https://t.co/YU4J2giAyt\",\"Testing keeps ramping up in the US.  A month ago it was roughly 100K tests per day, now it’s 300K per day.  And, yet, the trend in new cases of COVID are on a downward trend.  #OpenUpAmerica https://t.co/AZ5lgHTbaJ\",\"El procurador federal del Consumidor, Ricardo Sheffield, dijo -en su cuenta de Twitter- que ya se recuperó del coronavirus o Covid-19 del que estaba contagiado y señaló que retomaría sus labores. \\n(https://t.co/81kDaO2mcJ)\",\"Heads up: @QueenWillRock's 1992 Freddie Mercury Tribute Concert for AIDS Awareness is streaming right now to raise money for COVID-19 relief: https://t.co/kci28Urdkj #FreddieMercury https://t.co/ve782RdGo1\",\"Do you all think Florida is lying about numbers? \\n\\nFlorida and Georgia both are lax on everything and I feel like they just aren't reflecting the numbers of COVID anymore!\",\"Anyway, instead of listening to millionaire Matt Y’s vErY SeRiOuS PuNdiTmAn declaration that journalists are somehow too harsh when reporting on Trump’s insane comments, I wrote something that proves that the exact opposite is true https://t.co/NZTXeUsafk\",\"Former neurosurgeon says masks are ineffective against COVID-19 and can cause health problems https://t.co/J7oyWEdWPj\",\"Did y’all hear the piece on CNN that revealed if you’re in a restaurant with one who is asymptomatic &amp; they talk for 25 minutes they release 1000s of covid particles in the air that remain contagious for 12 minutes? Black news said that weeks ago but it was ignored. Stay safe\",\"BRAZIL REPORTS RECORD 13,944 COVID-19 CASES IN 24 HOURS\\n\\n(It’s 69 degrees with 87% humidity in Sao Paolo now)\\n\\n@business\",\"Dallas County could see summer surge in COVID-19 cases. According to UTSW scientists what happens is up to you. 69% overall compliance with #SaferAtHome and we knock cases down to only a few a day. 60% and we ballon to 800 a day by July. #FlattenTheCurve https://t.co/I0BCvC3RZH\",\"China has arrested nearly 500 people for speaking out about COVID-19, report says https://t.co/dIje3OTROE\",\"If you believe the number of COVID-19 deaths is accurate, you probably also believe the Left will be able to count mail-in-voting ballots correctly.\\n\\nAnd you're actually wrong on both counts.\",\"Joe Rogan: I might move to Texas 'if California continues to be this restrictive' with 'silly' COVID-19 shutdown https://t.co/JzebzCoPJa\",\"Dan Crenshaw sets the record straight on Trump's COVID response: 'I just want people to understand the truth' https://t.co/dUJnjIMUL9\",\"California biopharma company says it found coronavirus 'cure' that 100% blocks COVID-19 from infecting healthy cells https://t.co/xjtNKHYqLx\",\"Nursing homes that failed to protect residents from COVID-19 shielded by provision snuck into state budget https://t.co/kuWULK66JD\",\"What killed Sebastian Yellow? Coroner says the man drank himself to death with .550 blood alcohol reading-'ethanol toxicity' reads death certificate. But @CDPHE has categorized it as a #COVID19 death,  raising many questions. Our @CBSDenver report:\\nhttps://t.co/w5dHyT24wk https://t.co/viEClhYeBX\",\"This is promising! Some states are waking up to the fact that lying about the cause of death on official Death Certificates is not a good idea.\\nIllinois decided to remove inaccurate numbers from the COVID19 death toll, but did not go for full correction:\\nhttps://t.co/26UIWim6g3\",\"@elonmusk @TheZenCorner @stoolpresidente It's even worse than that Elon...\\n\\nMan found dead on the sidewalk with a .55 blood alcohol level and the Colorado Dept of Health made the Country Coroner categorize the death as Corona. This whole thing is a fraud!\\n\\nhttps://t.co/bMJ976DaMv\",\"NEW: A woman jailed in Colorado for what investigators describe as armed child kidnapping plot by QAnon believers, is asking to be released due to the COVID-19 pandemic, which QAnon conspiracy theorists have suggested is a hoax. #9NEWS #COVID19Colorado\",\"Vitamin D deficiency is scientifically proven to be associated with a lower immune system and a higher risk for viral diseases. So let's keep everyone indoors, away from sunlight and Vitamin D production, to prevent covid!\",\"“It's much more a question of care than of fear.” #Catholic #Covid https://t.co/fF5sCJamA9\",\"QAnon has grafted into conspiracy theories about COVID-19 after beginning with the claim that President Trump is fighting a secret war against a global pedophilia ring of Democrats who drink the blood of children. Which made for an awkward toss to our weather segment... https://t.co/awCrb4agmc\",\"The #AtlasV #USSF7 launch will honor all front-line workers and #COVID19 first responders while paying tribute to those affected by the pandemic #AmericaStrong\\n\\nhttps://t.co/7UFh0IFK34 https://t.co/GDkb55JLjb\",\"NEW: A Republican state representative is calling for criminal charges against Colorado's top public health official, accusing her of falsifying records to inflate COVID-19 death totals. #9NEWS #COVID19Colorado\",\"Not one scientific expert says there will be a vaccine in 2020.\\n\\nScientists are the guys who make the vaccines.\\n\\nAnd a rushed vaccine could mean a greater likelihood of unethical human testing, like the kind this bishop warns about. \\n\\nhttps://t.co/e8E7T1MEQR\",\"El Paso health director announces \\\"dramatic spike in positive cases\\\"\\nhttps://t.co/K1ALVrCs0D\",\"Inbox: Sen. Kamala Harris has led her colleagues in a letter to FEMA Administrator Gaynor and HHS Secretary Azar demanding an accurate death count and consistent statistics during the COVID-19 pandemic, after the Trump admin suggested the death toll might be lower than reported.\",\"India exports 50 million hydroxychloroquine tablets to U.S. for COVID-19 fight: source | Article [AMP] | Reuters https://t.co/LvTugornLr\",\"In episode 37 of Common Sense, we sit down with Dr. Steven Greer who provides a valuable perspective in analyzing our COVID-19 response.\\n\\nAs Doctor Greer states in today's episode, \\\"the goalpost keeps moving.\\\"\\n\\nhttps://t.co/MSo2ugUT1U https://t.co/1Ku6OEZWi1\",\"I will not stop fighting for our frontline workers just like they’re fighting for us\\n\\nI’ve been fighting for essential workers to get premium pay from a Heroes Fund\\n\\nAnd I’m proud $200B for this in the @HouseDemocrats’ bill\\n\\nSen. McConnell—We must act NOW\\n\\nhttps://t.co/TuDUHl0WZh\",\"As @HouseJudiciary Chair, I've made protecting the inmate population from COVID-19 a priority. I’m proud that the HEROES Act includes my bill to provide funds to keep our federal prisons safe and help state and local facilities stop the spread of COVID-19.\\nhttps://t.co/dWb53OqXLm\",\"We set prisoners free due to COVID fears, but killed the elderly by sending COVID into nursing homes! MADDENING!\\n#PROTECTOURSENIORS https://t.co/jbSHyOatlI\",\"NEW: NYC Health Dept announces big change in who should get tested.\\n\\n==&gt; Now it's everyone w/ symptoms, exposed to someone  with covid, or working in settings like nursing homes.\\n\\nFree walk-in testing available at 28 sites, find one here: https://t.co/cSc5hQqeIT\",\"Second Immigrant Dies of COVID-19 After Release from ICE Jail https://t.co/vvpUgB19Dk\",\"“Dr. Eric Topol, the director of the Scripps Research Translational Institute, who isn’t involved in the Seattle group, said it had ‘emerged as leading lights in this whole Covid-19 crisis.’ He said it was ‘bizarre’ the F.D.A. would halt such a project.\\nhttps://t.co/8fiv34peL2\",\"TRUMP says “FU Fauci” business &amp; schools will reopen. Will Fauci, who works for BIG PHARMA &amp; VACCINE pushers, RETALIATE with new  FAKE PANDEMIC? Is Fauci Dr Death of the economy?  No body wants to die. If you are afraid of Covid stay home, wear a mask &amp; LET THE REST OF US LIVE.\",\"Prayers to Art Howe, one of the nicest men who ever wore a baseball uniform, who's in intensive care in Houston with COVID-19.\",\"JUST IN: The number of COVID-19 cases in Arizona has jumped to 12,674. According to the latest numbers from ADHS, there have been 624 deaths due to the virus statewide.\\nLatest: https://t.co/S6OO1tEW0B\\n#ABC15 #Coronavirus https://t.co/TGO0MvLh3y\",\"On a scale of 1-1984, how are you feeling right now? Muzzles (masks, that is), arrows &amp; boxes on the floor telling you where you can &amp; can’t go, “show me your papers,” prove you’re essential, neighborly snitching, “wiretapping” of political enemies...#TruthNotFiction #COVID https://t.co/R8XmVcSN09\",\"If you idiots fall for this clear distraction of the fact that trump let 80k people die, 33 million go unemployed, and have the worst response to Covid than any other developed country in the world, I’m gonna scream https://t.co/cRYPfKq1mx\",\"From LA &amp; PHX to Asheville, NC &amp; WV, grassroots progressives did physically-distanced protests today to call for a #PeoplesBailoutBloc in the House to stop a vote on the Heroes Act unless paycheck guarantee, $2K monthly checks, &amp; free COVID treatment for all are added to bill. https://t.co/O3fI0GfDZH\",\"Criminals are cashing in on our demand for common household products. @CBP officers and @ICEgov #HSI agents across the country have intercepted Fake COVID-19 test kits, diluted bleach, and phony hand sanitizers. Watch my full story below\\n\\nhttps://t.co/ErXTOftxml\",\"COVID-19 originated in China. Multiple reports have stated that if China acted sooner and alerted the world, 95% of this could have been avoided.\\n\\nThe fact China wants to punish countries for accurately saying that shows they aren't the world's allies. https://t.co/A38kh0cRTA\",\"Physically-distanced protest at the AZ Dem Party HQ calling on @RubenGallego &amp; @gregstantonaz to join a progressive #PeoplesBailoutBloc and pledge to stop a vote on the Heroes Act unless paycheck protection, $2K monthly checks, &amp; free COVID treatment FOR ALL are added to bill. https://t.co/ORPitwqRDG\",\"TAKEAWAY Feds refuse to release model designed for Arizona that @drcarachrist &amp; @dougducey are relying on. But it’s a public document. https://t.co/K4JmkKBFjA via ⁦@azcentral⁩\",\"SCOOP: I obtained a whistleblower complaint about Stanford's COVID-19 study.\\n\\nTurns out JetBlue's founder, a critic of the economic shutdowns, helped fund it.\\n\\nAnd John Ioannidis and others allegedly ignored internal scientists' concerns about the test. \\n\\nhttps://t.co/dExNhZTVX0\",\"Important, please share!\\n\\nAnyone going in to work in San Francisco, or anyone who lives in SF and has one or more symptoms, can get a FREE COVID-19 test.\\n\\nNo insurance required, and you get results in 1-3 days.\\n\\nSign up today and #StopTheSpreadSF at https://t.co/VMsxpuvzTL https://t.co/z1VkVCme9U\",\"If we let police use drone surveillance with face recognition as a public health measure--we’ll likely see them flying over protests next year. https://t.co/dJ3L6U9Pne\",\"In the 30 yrs after WWII, working-class wages rose 91%.\\n\\nIn the last 40 years? Working-class wages were stagnant.\\n\\nNow Americans don’t have the savings they need to make it through COVID. Folks don’t just need jobs: they need a raise.\\n\\nLet's send $2k/month to start.\",\"Scammers are everywhere, even when you're job hunting.\\n\\nHere are 5 scam indicators you should be aware of if you're currently on the hunt, courtesy of @Forbes. 🕵️‍♀️🕵️‍♂️\\n https://t.co/CFgFQdGr8z\",\"There will be no “magic tech” solution to COVID-19. No silver bullet app will let us return to business as usual. App-assisted contact tracing will have serious limitations, and we don’t yet know the scope of the benefits. https://t.co/zngJHFZ8o7\",\"‘World’s largest concentrated solar plant’ (700 MW CSP + 250 MW PV) progresses despite Covid-19  \\n\\nThe $4.4 billion project is expected to increase the share of Dubai’s clean energy to 25% by 2030, allowing a saving of 1.6 million tonnes of carbon dioxide\\n\\nhttps://t.co/1LwDwTt3BU\",\"FDA warns about COVID-19 test used by White House after study shows it misses half of positive cases https://t.co/5ivhpjnS4h\",\"#GetTestedSF offers free COVID-19 tests to anyone who lives or works in #SF. No insurance required &amp; results are available within 1-3 days. Testing by appointment only &amp; can be made at: https://t.co/v2QlZ8U69V  #StopTheSpreadSF #ProtectOurCommunity https://t.co/31ezC0BPK2\",\"Saudi Arabia should #FreeLoujain immediately. \\n \\nNo prisoner of conscience should spend a day in jail. Particularly with the threat of COVID-19, pushing for gender equality should never be a death sentence. https://t.co/EQpKVtVIcy\",\"Nicaragua en medio de 4 pandemias: el covid-19, la pobreza, la ignorancia y la corrupción 🥺\",\"Wow. ⁦@RandPaul⁩ Chief Strategist doesn’t understand that having #COVID and being asymptomatic does not mean you will NOT develop serious COVID symptoms and/or die. https://t.co/xdqpMtuAaQ\",\"Gov Inslee: \\\"We should not be intimidated when people say, ‘Oh, you can’t use this COVID crisis, you know, to peddle a solution to climate change.’\\\"\\n\\nInslee doesn't even hide it: he will happily use the coronavirus for his radical, economy killing environmental agenda. https://t.co/yQfbwVh9Rh\",\"A resident of rural Snohomish County, \\\"Jean\\\" had COVID-like symptoms in late Dec 2019 and has subsequently tested positive in a serological assay. This may have been COVID-19 infection, but it's not certain (or even likely). 1/7\\nhttps://t.co/7GlZyAmwsu\",\"WATCH: Update on State Response to COVID-19 https://t.co/KS7HSYxION\",\"Here is a partial list of businesses in Washington State that have laid off employees, gone into bankruptcy, or closed permanently.\\n\\nIt's critical to keep track of COVID cases—but there should also be a public dashboard for jobs lost, companies shut down, and dreams demolished. https://t.co/snXlVchTA0\",\"For the first time in over two months, King County recorded no new deaths from COVID-19 in its daily update.\\n\\nhttps://t.co/QEnIGRBzhD\",\"As someone who’s had #COVID__19 to those who haven’t... please wear your god damn masks! \\n\\n#Masks4All #mask #StopTheSpread #COVID19 #COVID-19 #coronavirus #quarantinelife #SocialDistancing #Covid19usa #facemasks https://t.co/1ITluLQAsM\",\"Two Wash. residents who recall being sick with a respiratory illness in December have since had blood tests showing they developed antibodies for the coronavirus, raising questions about when the disease first began spreading here.\\n\\nhttps://t.co/qb8v6Et2Cd\",\"marble collection \\ntop two pair: $50 each\\nbottom two pair: $55 each \\n\\nincludes shipping. because of covid-19, i am not currently shipping to canada. sorry :( https://t.co/ssV6DpNHBf\"],\"retweet_count\":[776,707,291,281,281,141,124,88,87,45,242,56,51,48,35,35,33,32,29,25,282,213,166,139,93,85,70,56,48,48,177,175,111,97,85,51,50,48,42,35,455,351,216,216,149,73,65,44,43,38,822,399,81,71,69,65,57,42,35,34,983,233,71,70,48,46,37,35,35,33,410,239,230,204,78,76,72,49,48,34],\"hashtags\":[[null],[\"COVID19\"],[\"EncourageEsther\"],[\"COVID19\"],[\"AndrewCuomo\",\"Coronavirus\",\"NewYork\"],[null],[\"COVID19\"],[\"COVID19\"],[null],[\"COVID19\"],[null],[null],[null],[null],[null],[\"OpenUpAmerica\"],[null],[\"FreddieMercury\"],[null],[null],[null],[null],[null],[\"SaferAtHome\",\"FlattenTheCurve\"],[null],[null],[null],[null],[null],[null],[\"COVID19\"],[null],[null],[\"9NEWS\",\"COVID19Colorado\"],[null],[\"Catholic\",\"Covid\"],[null],[\"AtlasV\",\"USSF7\",\"COVID19\",\"AmericaStrong\"],[\"9NEWS\",\"COVID19Colorado\"],[null],[null],[null],[null],[null],[null],[null],[\"PROTECTOURSENIORS\"],[null],[null],[null],[null],[null],[\"ABC15\",\"Coronavirus\"],[\"TruthNotFiction\",\"COVID\"],[null],[\"PeoplesBailoutBloc\"],[\"HSI\"],[null],[\"PeoplesBailoutBloc\"],[null],[null],[\"StopTheSpreadSF\"],[null],[null],[null],[null],[null],[null],[\"GetTestedSF\",\"SF\",\"StopTheSpreadSF\",\"ProtectOurCommunity\"],[\"FreeLoujain\"],[null],[\"COVID\"],[null],[null],[null],[null],[null],[\"COVID__19\",\"Masks4All\",\"mask\",\"StopTheSpread\",\"COVID19\",\"COVID\",\"coronavirus\",\"quarantinelife\",\"SocialDistancing\",\"Covid19usa\",\"facemasks\"],[null],[null]],\"screen_name\":[\"MSharifpourMD\",\"CDCDirector\",\"BTSBEINGBTSYT\",\"CDCgov\",\"THEHermanCain\",\"lookner\",\"CDCgov\",\"CDCgov\",\"KiaSpeaks\",\"AmandaMurdie\",\"workwthecoach\",\"deneenknews\",\"WhiteHairedMC\",\"SharoneILJP\",\"RepBobbyRush\",\"wesbury\",\"_VicenteSerrano\",\"consequence\",\"SpeakUpNoworNev\",\"ParkerMolloy\",\"theblaze\",\"fhunscripted\",\"DiMartinoBooth\",\"JudgeClayJ\",\"theblaze\",\"TheDamaniFelder\",\"theblaze\",\"theblaze\",\"theblaze\",\"theblaze\",\"Briancbs4\",\"NikolovScience\",\"ReidarSundy\",\"KyleClark\",\"LivePDDave1\",\"cnalive\",\"KyleClark\",\"ulalaunch\",\"KyleClark\",\"jdflynn\",\"maddow\",\"kylegriffin1\",\"intheMatrixxx\",\"RudyGiuliani\",\"SenSchumer\",\"RepJerryNadler\",\"realPolitiDiva\",\"MarkLevineNYC\",\"democracynow\",\"biannagolodryga\",\"theRealKiyosaki\",\"BNightengale\",\"abc15\",\"kelliwardaz\",\"OGBobbyGabriel\",\"kai_newkirk\",\"StephBennettTV\",\"DanKEberhart\",\"kai_newkirk\",\"brahmresnik\",\"stephaniemlee\",\"LondonBreed\",\"EFF\",\"RepRoKhanna\",\"Chime\",\"EFF\",\"mzjacobson\",\"jilevin\",\"SF_emergency\",\"RepRoKhanna\",\"Fernanda__Mejia\",\"NaveedAJamali\",\"jasonrantz\",\"trvrb\",\"GovInslee\",\"realchrisrufo\",\"komonews\",\"Ryanintheus\",\"komonews\",\"N8VChey\"],\"created_at\":[\"2020-05-15T11:01:23\",\"2020-05-15T21:20:03\",\"2020-05-15T20:22:17\",\"2020-05-15T14:35:02\",\"2020-05-15T11:14:03\",\"2020-05-15T02:57:56\",\"2020-05-15T17:42:02\",\"2020-05-15T20:03:00\",\"2020-05-15T18:46:59\",\"2020-05-15T10:20:30\",\"2020-05-15T19:41:18\",\"2020-05-15T19:00:07\",\"2020-05-15T18:50:33\",\"2020-05-15T14:27:29\",\"2020-05-15T17:55:04\",\"2020-05-15T17:14:58\",\"2020-05-15T15:33:13\",\"2020-05-15T18:10:34\",\"2020-05-15T15:51:10\",\"2020-05-15T20:36:13\",\"2020-05-15T20:00:07\",\"2020-05-15T02:00:59\",\"2020-05-14T22:39:51\",\"2020-05-15T17:23:07\",\"2020-05-14T22:00:25\",\"2020-05-15T12:42:45\",\"2020-05-15T00:00:00\",\"2020-05-15T13:00:09\",\"2020-05-15T16:30:09\",\"2020-05-15T00:30:01\",\"2020-05-15T00:30:48\",\"2020-05-15T03:52:57\",\"2020-05-15T03:34:34\",\"2020-05-15T04:10:45\",\"2020-05-14T22:07:17\",\"2020-05-14T21:24:02\",\"2020-05-15T04:16:04\",\"2020-05-14T20:14:47\",\"2020-05-15T05:03:28\",\"2020-05-15T00:08:08\",\"2020-05-15T21:41:03\",\"2020-05-15T21:15:38\",\"2020-05-15T21:37:08\",\"2020-05-15T20:30:04\",\"2020-05-15T21:19:09\",\"2020-05-15T20:36:18\",\"2020-05-15T20:02:17\",\"2020-05-15T21:07:17\",\"2020-05-15T20:06:02\",\"2020-05-15T22:23:41\",\"2020-05-14T15:41:31\",\"2020-05-14T23:22:52\",\"2020-05-14T16:03:19\",\"2020-05-14T13:22:34\",\"2020-05-14T15:29:03\",\"2020-05-15T04:03:25\",\"2020-05-14T20:42:48\",\"2020-05-14T17:31:00\",\"2020-05-15T01:37:18\",\"2020-05-14T17:47:34\",\"2020-05-15T21:15:27\",\"2020-05-15T18:33:29\",\"2020-05-15T17:45:00\",\"2020-05-15T16:07:00\",\"2020-05-15T14:58:37\",\"2020-05-15T17:00:00\",\"2020-05-15T14:53:14\",\"2020-05-15T22:22:16\",\"2020-05-15T15:01:28\",\"2020-05-15T17:38:08\",\"2020-05-15T00:21:11\",\"2020-05-15T01:40:59\",\"2020-05-15T18:24:51\",\"2020-05-15T19:24:03\",\"2020-05-14T22:35:27\",\"2020-05-15T21:01:12\",\"2020-05-14T23:32:58\",\"2020-05-15T18:02:30\",\"2020-05-15T20:02:11\",\"2020-05-15T04:30:25\"],\"favorite_count\":[1924,589,544,361,635,216,168,123,125,64,859,83,215,59,104,132,107,54,183,134,531,578,586,282,150,362,294,157,116,55,208,298,535,402,166,146,259,253,154,123,770,1032,383,446,520,206,94,84,43,50,3630,2297,125,125,177,157,70,130,70,55,1603,309,110,286,187,71,76,27,30,106,1004,811,495,482,260,106,311,71,106,99]},\"columns\":[{\"accessor\":\"city_searched\",\"name\":\"city_searched\",\"type\":\"character\"},{\"accessor\":\"text\",\"name\":\"text\",\"type\":\"character\",\"minWidth\":400,\"className\":\"text-col\"},{\"accessor\":\"retweet_count\",\"name\":\"retweet_count\",\"type\":\"numeric\",\"style\":[{\"background\":\"#B7B7D1\"},{\"background\":\"#BDBDD5\"},{\"background\":\"#DFDFEB\"},{\"background\":\"#E0E0EC\"},{\"background\":\"#E0E0EC\"},{\"background\":\"#EBEBF4\"},{\"background\":\"#EDEDF5\"},{\"background\":\"#F0F0F6\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#E3E3EE\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F3F3F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4FA\"},{\"background\":\"#F4F4FA\"},{\"background\":\"#F5F5FA\"},{\"background\":\"#E0E0EC\"},{\"background\":\"#E5E5F0\"},{\"background\":\"#E9E9F2\"},{\"background\":\"#EBEBF4\"},{\"background\":\"#EFEFF6\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#E8E8F2\"},{\"background\":\"#E9E9F2\"},{\"background\":\"#EEEEF5\"},{\"background\":\"#EFEFF6\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F3F3F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#D2D2E3\"},{\"background\":\"#DADAE8\"},{\"background\":\"#E5E5F0\"},{\"background\":\"#E5E5F0\"},{\"background\":\"#EBEBF3\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#B3B3CF\"},{\"background\":\"#D6D6E6\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F8\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F2F2F8\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#A6A6C6\"},{\"background\":\"#E4E4EF\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#F4F4F9\"},{\"background\":\"#D5D5E5\"},{\"background\":\"#E3E3EE\"},{\"background\":\"#E4E4EF\"},{\"background\":\"#E6E6F0\"},{\"background\":\"#F0F0F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F1F1F7\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F3F3F9\"},{\"background\":\"#F4F4F9\"}]},{\"accessor\":\"hashtags\",\"name\":\"hashtags\",\"type\":\"list\",\"style\":{\"borderLeft\":\"2px solid #8585ad\"}},{\"accessor\":\"screen_name\",\"name\":\"screen_name\",\"type\":\"character\"},{\"accessor\":\"created_at\",\"name\":\"created_at\",\"type\":\"Date\"},{\"accessor\":\"favorite_count\",\"name\":\"favorite_count\",\"type\":\"numeric\"}],\"pivotBy\":[\"city_searched\"],\"filterable\":true,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"className\":\"tweet-tbl\",\"height\":\"600px\",\"dataKey\":\"57a479eeb56b3afd0bca8c585f649ae4\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\n\n\n\n",
    "preview": "posts/2020-05-14-twitter-by-location/tweet_table.png",
    "last_modified": "2021-02-15T22:50:29-05:00",
    "input_file": {},
    "preview_width": 2418,
    "preview_height": 1494
  },
  {
    "path": "posts/2020-05-12-scrape-and-clean-an-online-table/",
    "title": "Scrape, Clean, Join, Plot",
    "description": "Write a function to scrape an online table, then use data.table to prep it for analysis. Use text from the table to add text annotations in a ggplot2 bar plot.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-12",
    "categories": [],
    "contents": "\nThe problem: add date annotations to a bar graph of COVID-19 deaths, but those dates are in an online table, and we have to join those dates onto another data set.\nWe will use the rvest package to scrape a table from the web, then the stringr and data.table packages to clean it up and prep it for use in ggplot2.\nThe dates table will come from Ballotpedia\nThe COVID-19 deaths data will come from The New York Times\n\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(data.table)\n\n# define a function that can scrape a table from the web\nget_html_table <- function(url, xpath, header = NA) {\n  \n  url %>%\n    read_html() %>% # download page source\n    html_nodes(xpath = xpath) %>%\n    html_table(header = header) %>% # extract html table\n    .[[1]] %>% # get dataframe from list\n    as.data.table()\n\n}\n\n\n\n# set variables for url and xpath\n# spliting the url here only for printing-width purposes on the blog\nhalf1 <- \"https://ballotpedia.org/States_with_lockdown_and_stay-at-home_orders\"\nhalf2 <- \"_in_response_to_the_coronavirus_(COVID-19)_pandemic,_2020\"\nmy_url <- paste0(half1, half2)\n\nmy_xpath <- '//*[@id=\"mw-content-text\"]/table[1]'\n\n# get the table\nstay_at_home_table <- get_html_table(\n  url = my_url,\n  xpath = my_xpath,\n  header = FALSE\n)\n\n\nTo get the XPath for the table, go to the site, right-click anywhere on the table and select Inspect Element. You should then see the table in the html. Right-click the table in the html and select Copy, then XPath. I’m on a Mac in Safari, so this may look a little different if you are on Linux or Windows or other browsers.\n\n\n\n\nRow 2 actually has the column names we want.\n\n\n# extract row 2 values as a vector for use as column names\ncol_names <- stay_at_home_table[2, paste(.SD)] %>%\n  str_to_lower() %>%\n  str_replace_all(\" \", \"_\")\n\n# keep rows 3 through the end, set new column names, drop link column\nstay_at_home_table <- stay_at_home_table[3:.N]\nsetnames(stay_at_home_table, col_names)\nstay_at_home_table[, link_to_order := NULL]\n\nstay_at_home_table[1]\n\n     state       order_dates            official_name_of_order\n1: Alabama April 4- April 30 Suspend certain public gatherings\n\n\nWe need two date columns, one for the start dates and another column for the end dates. Unfortunately, the table authors combine this data in a single column. We will split the existing date column into two using tstrsplit() from the data.table package. First, we will remove the citations from the dates, then split the date column into two. We finish up by handling the various types of missing data we find.\n\n\n# clean clean clean\nstay_at_home_table[ # remove citations\n  , order_dates := str_remove_all(order_dates, \"\\\\[..?\\\\]\")\n][ # fix Alaska dates\n  state == \"Alaska\", order_dates := \"March 28 - TBD\"\n][ # split date column into two columns\n  , c(\"start\", \"end\") := tstrsplit(order_dates, \"-\", fixed = TRUE)\n][ # trim white space from both columns\n  , c(\"start\", \"end\") := lapply(.SD, str_trim), .SDcols = c(\"start\", \"end\")\n][ # drop the original date column\n  , order_dates := NULL\n][ # replace none and tbd with NA\n  , c(\"start\", \"end\") := lapply(.SD, function(x) fifelse(x %in% c(\"None\", \"TBD\"), NA_character_, x))\n  , .SDcols = c(\"start\", \"end\")\n][ # in other column replace NA string with word None\n  official_name_of_order == \"N/A\", official_name_of_order := \"None\"\n][ # add year to dates so we can convert them\n  , c(\"start\", \"end\") := lapply(.SD, function(x) fifelse(!is.na(x), paste(x, \"2020\"), x)),\n  , .SDcols = c(\"start\", \"end\")\n][# then convert all to proper date class\n  , c(\"start\", \"end\") := lapply(.SD, anytime::anydate), .SDcols = c(\"start\", \"end\")\n]\n\nstay_at_home_table[1]\n\n     state            official_name_of_order      start        end\n1: Alabama Suspend certain public gatherings 2020-04-04 2020-04-30\n\n\nGet the COVID-19 data using the fast fread() function from data.table, and also sum up the total cases and deaths and get the date range of the data.\n\n\n# get state data from The New York Times github\nstates <- fread(\n  input = \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\",\n  colClasses = c(date = 'IDate'),\n  key = c('state', 'date')\n)\n\n# get national data from The New York Times github\nus <- fread(\n  input = \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv\",\n  colClasses = c(date = 'IDate'),\n  key = 'date'\n)\n\n# total cases in the US\nus_cases <- us[, last(cases)] %>% \n  formatC(digits = 0, format = \"d\", big.mark = \",\")\n\n# total deaths in the US\nus_deaths <- us[, last(deaths)] %>% \n  formatC(digits = 0, format = \"d\", big.mark = \",\")\n\n# date range of the data\ndate_range <- states[, range(unique(date))]\nnames(date_range) <- c(\"first\", \"last\")\n\n\nBuild our data for the plot. We’ll make 7-day rolling averages for cases and deaths. The rolling averages will smooth over reporting inconsistencies, for example, when a hospital or county may report all weekend deaths on a Monday. Then we will join the state’s closing and reopening dates onto our COVID counts for each state.\n\n\nplot_dat <- states[\n  , `:=`( # get new cases and deaths per day by subtracting previous day (lag)\n      new_cases =  cases - shift(cases,  type = \"lag\", n = 1L),\n      new_deaths = deaths -shift(deaths, type = \"lag\", n = 1L)\n    )\n    , by = state\n  ][ # rolling avergaes for new cases and new deaths by state\n    , `:=`(\n      nc_avg = frollmean(new_cases,  7L, align = \"right\"),\n      nd_avg = frollmean(new_deaths, 7L, align = \"right\")\n    )\n    , by = state\n  ][ # round them all up using ceiling\n    , c(\"nc_avg\", \"nd_avg\") := lapply(.SD, ceiling), .SDcols = c(\"nc_avg\", \"nd_avg\")\n  ][ # join the closing and reopening dates\n    stay_at_home_table, on = \"state\"\n  ][\n    , plot_label_end := \"REOPEN\"\n  ][\n    , plot_label_start := \"CLOSE\"\n  ]\n\n\nPlotting all the data for each state would be too much for this blog post. Instead, I’ll pick two states and plot their daily death counts, 7-day rolling averages, and closing and opening dates.\n\n\n# colors\nmain_blue <- \"#abbfd3\"\naccent_light <- \"#eef2f6\"\naccent_dark <- \"#698cb0\"\ngrey_dark <- \"#627384\"\nrestriction_red <- \"#f47171\"\nreopen_green <- \"#24a8a8\"\n\n# max date for x axis\nmax_plot_date <- max(\n  Sys.Date(),\n  plot_dat[state %chin% c(\"Texas\", \"Michigan\"),\n           max(end, na.rm = TRUE)]\n)\n\nplot_dat[state %chin% c(\"Texas\", \"Michigan\"), ] %>% \n      ggplot(aes(x = date, y = new_deaths)) + \n      geom_bar(stat = \"identity\",\n               fill = main_blue,\n               color = accent_light,\n               width = 1,\n               alpha = .4) +\n      geom_smooth(aes(x = date, y = nd_avg),\n                  method = \"loess\",\n                  color = accent_dark,\n                  se = FALSE,\n                  size = .5) +\n      geom_text(aes(x = as.Date(\"2020-03-11\"), y = 0.01,\n                    label = \"PANDEMIC 3/11\"),\n                size = 2, angle = 90, vjust = 0, hjust = 0, color = grey_dark) +\n      geom_text(aes(x = start, y = 0.01, label = plot_label_start),\n                size = 2, angle = 90, vjust = 0, hjust = 0, color = restriction_red) +\n      geom_text(aes(x = end, y = 0.01, label = plot_label_end),\n               size = 2, angle = 90, vjust = 0, hjust = 0, color = reopen_green) +\n      scale_x_date(limits = c(as.Date(\"2020-03-10\"), max_plot_date), breaks = \"1 week\") +\n      facet_wrap(~state, scales = \"free\",\n                 ncol = 1) +\n      labs(y = \"\",\n           x = \"\",\n           title = \"New Deaths by Day Since March 11th 2020\",\n           subtitle = \"on March 11th, 2020 the WHO declared the coronavirus outbreak a global pandemic\",\n           caption = \"Dataviz by Jeremy Allen | COVID data by The New York Time | Open Close Dates by Ballotpedia\") +\n      theme(\n        axis.ticks = element_line(color = accent_dark, size = .3),\n        axis.line.x = element_line(color = accent_dark, size = .3),\n        axis.text = element_text(color = \"#627384\", size = 8),\n        axis.text.x = element_text(angle = 90, vjust = .5),\n        axis.title = element_text(color = \"#627384\", size = 8),\n        panel.background = element_rect(fill = accent_light, color = accent_light),\n        plot.background = element_rect(fill = accent_light, color = accent_light),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = accent_light, color = accent_light),\n        strip.text = element_text(color = \"#627384\", size = 12, hjust = 0,\n                                  margin = unit(c(10,10,10,10), \"pt\")),\n        plot.title = element_text(color = \"#627384\", size = 20,\n                                  margin = unit(c(2,10,5,0), \"pt\")),\n        plot.subtitle = element_text(color = \"#627384\", size = 14,\n                                     margin = unit(c(5,10,20,0), \"pt\")),\n        plot.caption = element_text(color = \"#627384\", size = 6),\n        plot.margin = unit(c(15,40,15,40), \"pt\"),\n        panel.spacing = unit(70, \"pt\")\n      )\n\n\n\n\n",
    "preview": "posts/2020-05-12-scrape-and-clean-an-online-table/scrape-and-clean-an-online-table_files/figure-html5/plot-1.png",
    "last_modified": "2020-05-13T01:20:47-04:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1152
  },
  {
    "path": "posts/2020-05-11-find-index-of-first-instance/",
    "title": "Find Index of First Instance",
    "description": "Find one thing with another thing. We'll speed test various data.table and tidyverse methods for finding the position of the first match and use that to index another column.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-11",
    "categories": [],
    "contents": "\nYes, another data.table and tidyverse speed test, but this is more than that! I swear. This is real code of me working through a specific real-world issue.\nThe problem: Get the position number of the first instance of a thing in one column, and use that number to pick a thing from another column, in my case, returning the first date on which a specified number of cases occurred.\nLoad packages and make some fake data.\n\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(purrr)\n\n# lots of dates\ndate = seq.Date(from = as.Date(\"1900-01-01\"),\n                to = as.Date(\"2900-12-31\"),\n                by = \"day\")\n\n# lots of cases\ncases = c(1:length(date)-1)\n\n# make a dataframe\ndf <- data.frame(date = date, cases = cases)\n\n\nLet’s use which.max() to get the date on which the first instance of 10,0000 cases occurred.\n\n\n# Get the position of the first instance of 10,000 in the cases col,\n# and use that number to index the date col, returning the first date\n# on which 10,000 cases occurred.\ndt <- as.data.table(df) # convert to data.table first\ndt[, date[which.max(cases >= 10000)]]\n\n[1] \"1927-05-20\"\n\n# this only works because 10000 is a vlaue that can be found in that column.\n\n\nHowever, which.max() returns 1 when it fails, thus indexing our first date, which we do not want because there are no days with 400,000 or more cases.\nWe need NA returned when we don’t find an instance of the value we are looking for.\n\n\n# which.max returns 1 when it fails, thus indexing\n# our first date, which we do not want because there\n# are no days with 400,000 or more cases. We expect NA.\ndt[, date[which.max(cases >= 400000)]]\n\n[1] \"1900-01-01\"\n\n\nLet’s test many methods. We want to speed test them at the end, so I’m putting each method inside a function because it’s easier to add them as functions in the speed test once we get there.\n\n\n# which.max(), does NOT return NA when it fails. Bad.\ndt_which_max_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[which.max(cases >= 400000)]]\n} \n\n# match(true, x) will return NA when it fails, which\n# is what we want so that we don't get a date returned\n# when there are no days with 400,000 or more cases\ndt_match_true_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[match(TRUE, cases >= 100000)]]\n} \n\n# which()[1], test them all and return the first one, also returns NA\ndt_which_first_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[which(cases >= 400000)[1L]]]  \n}\n\n# use base R's Position function, also returns NA\ndt_position_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[Position(function(x) x >= 400000, cases)]]\n}\n\n# Tidyverse's purrr::detect_index(), returns 'Date of length 0'\ntv_purrr_method <- function() {\n  tb <- tibble::as_tibble(df)\n  tb %>%\n    slice(purrr::detect_index(cases, ~.x >= 400000)) %>% \n    pull(date)\n}\n\n# Tidyverse mixed with the base R's match function\ntv_match_method <- function() {\n  tb <- tibble::as_tibble(df)\n  tb %>%\n    slice(match(TRUE, cases >= 100000)) %>% \n    pull(date)\n}\n\n\nGet each function into microbenchmark and test each one 100 times.\n\n\n#--- Speed test them each 100 times\n\nmicrobenchmark::microbenchmark(\n  dt_which_max_method(),\n  dt_match_true_method(),\n  dt_which_first_method(),\n  dt_position_method(),\n  tv_purrr_method(),\n  tv_match_method(),\n  times = 100L\n)\n\nUnit: milliseconds\n                    expr        min         lq       mean     median\n   dt_which_max_method()   1.945617   2.426501   3.481390   2.705386\n  dt_match_true_method()   1.822808   2.400708   3.650070   2.729901\n dt_which_first_method()   1.945631   2.366789   3.678050   2.605177\n    dt_position_method() 160.590374 178.364530 188.521428 184.895142\n       tv_purrr_method() 785.695242 901.754074 947.888644 931.474398\n       tv_match_method()   1.444482   1.897420   3.139463   2.094831\n         uq        max neval\n   3.306638   10.57839   100\n   3.797825   16.06448   100\n   4.860582   10.31487   100\n 194.051222  265.28539   100\n 977.642047 1403.64966   100\n   2.812000   67.40537   100\n\n\nThe vectorized methods, such as match() used on either a data.table or a tidyverse tibble are clear winners over the base Position() and purrr detect_index() functions.\n\n\n",
    "preview": "posts/2020-05-11-find-index-of-first-instance/index_benchmark.png",
    "last_modified": "2020-05-12T22:20:42-04:00",
    "input_file": {},
    "preview_width": 1514,
    "preview_height": 328
  },
  {
    "path": "posts/2020-05-01-order-months/",
    "title": "Order Months",
    "description": "Produce a vector of month names ending with the current month.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\nThe last 12 months with order_months()\nI frequently have to make tables or plots of events that “happened in the last 12 months.” I like to generate a character vector of month names for the last 12 months which I can then use for ordered factor levels, factor labels, column names, or x-axis labels.\nThis helper function will produce a character vector of names of the last 12 months ending either at the current month or a given month. The current month or given month will be the last element in the vector, with the preceding 11 months before it.\nIf no month number is given for x, the current month is used. Given months must be given as a number. Month names can be returned as abbreviated or full names. Set label = \"abb\" or label = \"names\". X must be 1-12.\nRemember, the current or given month is always at the end of the vector!\n\n\norder_months <- function(x = NULL, label = \"abb\") {\n  \n  # This function takes a given month number or the current month\n  # number and returns a character vector of the last 12 months,\n  # including current month. For example, if it is now February: \n  # \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\" \"Jan\" \"Feb\"\n  # is returned with the current month at the end.\n  # This makes a nice x axis if you need to plot something for\n  # the \"last 12 months\".\n  # if x is NULL the current month is taken from Sys.Date\n  # if x is given, x must be 1-12\n  # if label is \"abb\", month names are abbreviated\n  # if label is \"names\", month names are full\n  \n  # stop if x is out of bounds\n  if(!is.null(x))\n    if(x > 12 | x < 1) stop(\"x must be 1-12\")\n  \n  # sets of named integers using abbreviated and full month names\n  months_abb <- setNames(1:12, month.abb)\n  months_names <- setNames(1:12, month.name)\n  \n  # current month number\n  m <- as.POSIXlt(Sys.Date())$mon + 1\n  \n  # get a value for x\n  if(is.null(x)) x <- m + 1 else x <- x + 1\n  \n  # a and b components for main if\n  # if x is 1 or 12\n  a <- 1:12\n  # if x is 2 through 12\n  b <- c(\n    x:12,\n    1:(x-1)\n    )\n  \n  # main if\n  if(x == 1 | x == 13) new_order <- a else new_order <- b\n\n  # use new_order to set desired order of months\n  if(label == \"abb\") my_month_order <- names(months_abb[new_order])\n  if(label == \"names\") my_month_order <- names(months_names[new_order])\n  \n  my_month_order\n}\n\nTypical usage\n\n\norder_months()\n\n [1] \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\" \"Jan\" \"Feb\" \"Mar\" \"Apr\"\n[12] \"May\"\n\n\n\norder_months(label = \"names\")\n\n [1] \"June\"      \"July\"      \"August\"    \"September\" \"October\"  \n [6] \"November\"  \"December\"  \"January\"   \"February\"  \"March\"    \n[11] \"April\"     \"May\"      \n\n\n\norder_months(x = 5, label = \"abb\")\n\n [1] \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\" \"Jan\" \"Feb\" \"Mar\" \"Apr\"\n[12] \"May\"\n\n\n\n",
    "preview": "posts/2020-05-01-order-months/order_months.png",
    "last_modified": "2020-05-12T17:01:29-04:00",
    "input_file": {},
    "preview_width": 1366,
    "preview_height": 332
  }
]
