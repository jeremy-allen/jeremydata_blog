[
  {
    "path": "posts/2021-02-15-shiny-holiday-party/",
    "title": "Shiny Holiday Party",
    "description": "Fun Shiny app to draw names at the office party",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "shiny",
      "fun"
    ],
    "contents": "\nFor an annual holiday Zoom party I was asked to draw a random team member’s name who would win a prize. So I made a Shiny app! The code is on my Github if you want it. The app is on shinyapps.io. if you want to play.\nThe Fun Part\nIn addition to picking a name, the app also searches IMDB movie plot summaries for that name, picks a random one from the results and shows it in the app without the movie title. Then everyone tries to guess the movie title. There is a “reveal” button to reveal the movie title if no one can guess.\nI removed my coworkers’ names from the app and am using a random selection from the RStudio team page. Here’s a preview, now go play!\n\n\n\n\n",
    "preview": "posts/2021-02-15-shiny-holiday-party/site_screenshot.png",
    "last_modified": "2021-02-15T22:44:31-05:00",
    "input_file": "shiny-holiday-party.utf8.md",
    "preview_width": 1998,
    "preview_height": 1824
  },
  {
    "path": "posts/2020-05-17-twitter-bookmarks/",
    "title": "Twitter Bookmarks",
    "description": "Recently bookmarked tweets worth sharing.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-17",
    "categories": [],
    "contents": "\nRecently bookmarked tweets worth sharing\nClick an image to go there!\n\n\n\ndiv.l-screen {\n  overflow-x: scroll;\n}\n\n\n\n\n\ntweets_map_layers\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\n\n\n",
    "preview": "posts/2020-05-17-twitter-bookmarks/preview.png",
    "last_modified": "2020-05-17T14:04:29-04:00",
    "input_file": {},
    "preview_width": 3782,
    "preview_height": 1098
  },
  {
    "path": "posts/2020-05-14-twitter-by-location/",
    "title": "Twitter by Location",
    "description": "Make functions that will gather tweets by keyword in multiple geographic locations, find the most resonating and most activating tweets in each city and present them in a pretty table for reading",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-14",
    "categories": [
      "twitter",
      "web scraping",
      "api",
      "geocoding",
      "tables"
    ],
    "contents": "\n\nProblem: What are Twitter users saying about COVID-19 in different cities across the US?\n\nLoad our packages. rtweet retrieves our tweets from the Twitter API and also gets the latitude and longitude coordinates for our cities by sending our city names through the Google Maps API. We will use the reactable package to make beautiful custom tables to read our top tweets in each city.\n\n\n# do we need to get more tweets?\nneed_online_tweets <- FALSE\nif(need_online_tweets == FALSE) {\n  tweets <- readRDS(fs::dir_ls(here::here(\"_posts\",\n                               \"2020-05-14-twitter-by-location\"),\n                               regexp = \".*covid_tweets.*\"))\n} else { # download more tweets:\n\n#---- some helper functions for later\n  \n# search twitter\nget_tweets <- function(locs, query) {\n  \n  search_tweets(\n    q = query,\n    geocode = locs,\n    n = 2000,\n    include_rts = FALSE\n  )\n  \n}\n\n# get coordinates for the places we want to search\nget_us_coords <- function(place, mile_radius) {\n  \n  # function output should be a character vetcor \n  # that looks like: \"40.397408,-102.054770,130mi\"\n  # with no spaces, which is the format required by\n  # the twitter api\n  \n  if(!inherits(place, \"character\")) {\n    stop (\"place must be a quoted character string\")\n  }\n  \n  if(!inherits(mile_radius, \"character\")) {\n    stop (\"mile_radius must be a quoted character string\")\n  }\n\n  # requires google maps api key\n  rtweet::lookup_coords(\n    address = place,\n    components = \"country:US\",\n    apikey = Sys.getenv(\"GOOGLE_MAPS_KEY\")\n    ) %>%\n    .[[\"point\"]] %>%\n    paste0(., collapse = \",\") %>%\n    paste0(., \",\", mile_radius, \"mi\")\n  \n}\n\n#--- now to using those functions to get the tweets we want\n\n# list of places to search for tweets\nmy_places <- list(\n  \"New York, New York\",\n  \"Atlanta, Georgia\",\n  \"Chicago, Illinois\",\n  \"Dallas, Texas\",\n  \"Denver, Colorado\",\n  \"Phoenix, Arizona\",\n  \"Seattle, Washington\",\n  \"San Francisco, California\"\n)\n\n# map our get_us_coords() function over our list of places to get a list of\n# lat long points with bundled radius for each place\nlocs <- purrr::map(\n  my_places,\n  get_us_coords,\n  mile_radius = \"55\"\n)\n\ntime_of_search <- Sys.time()\n\n# map our get_tweets() function over our list of places\n# then combine results and remove duplicate tweets\ndat_list <- purrr::map(\n  .x = locs,\n  .f = get_tweets,\n  query = \"COVID\"\n) %>% # name each list element with its location name, to use in bind_rows below\n  set_names(my_places)\n\ndat <- bind_rows(\n  dat_list,\n  .id = \"city_searched\" # this column will be filled with the list element names\n) %>% unique()\n\n\ntweets <- dat %>% \n  select( # reorder columns\n    city_searched,\n    location,\n    created_at,\n    screen_name,\n    name,\n    text,\n    hashtags,\n    favorite_count,\n    retweet_count,\n    quote_count,\n    reply_count,\n    status_id,\n    everything()\n  ) %>% # the geo_coords column is a list column of lat-lng vectors, so unnest_wider\n  mutate(geo_coords = map(geo_coords, ~set_names(., c(\"lat\", \"lng\")))) %>% \n  unnest_wider(geo_coords) %>% \n  arrange(\n    desc(created_at)\n  )\n\n#---- write the tweets to disk\n\n# format time for use in file name\nmy_time <- str_replace_all(time_of_search, \" \", \"_\")\nmy_time <- str_replace_all(my_time, \":\", \".\")\nmy_file_name <- paste0(\"covid_tweets_\", my_time)\n\n# write tweets to disk\nsaveRDS(tweets, here::here(my_file_name))\n#rio::export(tweets, here::here(my_file_name))\n\n}\n\n\n\n\nGroup by city, arrange the favorite_count column in descending order, and limit the columns we display.\n\n\n# favorited\nmost_resonating <- tweets %>% \n  select(city_searched,\n         text,\n         favorite_count,\n         hashtags,\n         screen_name,\n         created_at,\n         retweet_count) %>% \n  group_by(city_searched) %>%\n  arrange(desc(favorite_count)) %>% \n  slice(1:10)\n  \n\n# retweeted\nmost_activating <- tweets %>% \n  select(city_searched,\n         text,\n         retweet_count,\n         hashtags,\n         screen_name,\n         created_at,\n         favorite_count\n         ) %>%\n  group_by(city_searched) %>%\n  arrange(desc(retweet_count)) %>% \n  slice(1:10)\n\n\n\n\nSet some basic CSS for our table class and the class of our text column.\n\n\n.tweet-tbl {\n  font-size: .6em;\n}\n\n.text-col {\n  font-weight: 600;\n  color: #e96384;\n}\n\n\n.tweet-tbl {\n  font-size: .6em;\n}\n\n.text-col {\n  font-weight: 600;\n  color: #e96384;\n}\n\n\n\nThe table showing the top 10 most favorited tweets in each city. Click a city to expand its rows. Scroll right to see more columns. First up are the tweets that resonate the most, i.e., the tweets most favorited.\n\n\n# a color palette function to shade table cells based on a value\nmy_pal <- function(x) rgb(colorRamp(c(\"#f0f0f5\", \"#a3a3c2\"))(x), maxColorValue = 250)\n\nresonate_table <- reactable(\n  most_resonating,\n  groupBy = \"city_searched\",\n  filterable = TRUE,\n  class = \"tweet-tbl\",\n  height = 600,\n  columns = list(\n    text = colDef(\n      class = \"text-col\", # see CSS chunk for styling\n      minWidth = 400\n    ),\n    favorite_count = colDef(style = function(value) {\n      normalized <- (value - min(most_resonating$favorite_count)) / \n        (max(most_resonating$favorite_count) - min(most_resonating$favorite_count))\n      color <- my_pal(normalized)\n      list(background = color)\n      }\n    ),\n    hashtags = colDef(style = list(borderLeft = \"2px solid #8585ad\"))\n  )\n)\n\n\n\nTop 10 most resonating tweets in each city\n\npreserve37b2bd29cea0a941\n\n\nNext up are the tweets that activate the most, i.e., the tweets most retweeted.\n\n\nactivate_table <- reactable(\n  most_activating,\n  groupBy = \"city_searched\",\n  filterable = TRUE,\n  class = \"tweet-tbl\",\n  height = 600,\n  columns = list(\n    text = colDef(\n      class = \"text-col\", # see CSS chunk for styling\n      minWidth = 400\n    ),\n    retweet_count = colDef(style = function(value) {\n      normalized <- (value - min(most_activating$retweet_count)) / \n        (max(most_activating$retweet_count) - min(most_activating$retweet_count))\n      color <- my_pal(normalized)\n      list(background = color)\n      }\n    ),\n    hashtags = colDef(style = list(borderLeft = \"2px solid #8585ad\"))\n  )\n)\n\n\n\nTop 10 most activating tweets in each city\n\npreservec290cd23323790e2\n\n\n\n\n",
    "preview": "posts/2020-05-14-twitter-by-location/tweet_table.png",
    "last_modified": "2021-02-15T22:50:28-05:00",
    "input_file": "twitter-by-location.utf8.md",
    "preview_width": 2418,
    "preview_height": 1494
  },
  {
    "path": "posts/2020-05-12-scrape-and-clean-an-online-table/",
    "title": "Scrape, Clean, Join, Plot",
    "description": "Write a function to scrape an online table, then use data.table to prep it for analysis. Use text from the table to add text annotations in a ggplot2 bar plot.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-12",
    "categories": [],
    "contents": "\nThe problem: add date annotations to a bar graph of COVID-19 deaths, but those dates are in an online table, and we have to join those dates onto another data set.\nWe will use the rvest package to scrape a table from the web, then the stringr and data.table packages to clean it up and prep it for use in ggplot2.\nThe dates table will come from Ballotpedia\nThe COVID-19 deaths data will come from The New York Times\n\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(data.table)\n\n# define a function that can scrape a table from the web\nget_html_table <- function(url, xpath, header = NA) {\n  \n  url %>%\n    read_html() %>% # download page source\n    html_nodes(xpath = xpath) %>%\n    html_table(header = header) %>% # extract html table\n    .[[1]] %>% # get dataframe from list\n    as.data.table()\n\n}\n\n\n\n# set variables for url and xpath\n# spliting the url here only for printing-width purposes on the blog\nhalf1 <- \"https://ballotpedia.org/States_with_lockdown_and_stay-at-home_orders\"\nhalf2 <- \"_in_response_to_the_coronavirus_(COVID-19)_pandemic,_2020\"\nmy_url <- paste0(half1, half2)\n\nmy_xpath <- '//*[@id=\"mw-content-text\"]/table[1]'\n\n# get the table\nstay_at_home_table <- get_html_table(\n  url = my_url,\n  xpath = my_xpath,\n  header = FALSE\n)\n\n\nTo get the XPath for the table, go to the site, right-click anywhere on the table and select Inspect Element. You should then see the table in the html. Right-click the table in the html and select Copy, then XPath. I’m on a Mac in Safari, so this may look a little different if you are on Linux or Windows or other browsers.\n\n\n\n\nRow 2 actually has the column names we want.\n\n\n# extract row 2 values as a vector for use as column names\ncol_names <- stay_at_home_table[2, paste(.SD)] %>%\n  str_to_lower() %>%\n  str_replace_all(\" \", \"_\")\n\n# keep rows 3 through the end, set new column names, drop link column\nstay_at_home_table <- stay_at_home_table[3:.N]\nsetnames(stay_at_home_table, col_names)\nstay_at_home_table[, link_to_order := NULL]\n\nstay_at_home_table[1]\n\n     state       order_dates            official_name_of_order\n1: Alabama April 4- April 30 Suspend certain public gatherings\n\n\nWe need two date columns, one for the start dates and another column for the end dates. Unfortunately, the table authors combine this data in a single column. We will split the existing date column into two using tstrsplit() from the data.table package. First, we will remove the citations from the dates, then split the date column into two. We finish up by handling the various types of missing data we find.\n\n\n# clean clean clean\nstay_at_home_table[ # remove citations\n  , order_dates := str_remove_all(order_dates, \"\\\\[..?\\\\]\")\n][ # fix Alaska dates\n  state == \"Alaska\", order_dates := \"March 28 - TBD\"\n][ # split date column into two columns\n  , c(\"start\", \"end\") := tstrsplit(order_dates, \"-\", fixed = TRUE)\n][ # trim white space from both columns\n  , c(\"start\", \"end\") := lapply(.SD, str_trim), .SDcols = c(\"start\", \"end\")\n][ # drop the original date column\n  , order_dates := NULL\n][ # replace none and tbd with NA\n  , c(\"start\", \"end\") := lapply(.SD, function(x) fifelse(x %in% c(\"None\", \"TBD\"), NA_character_, x))\n  , .SDcols = c(\"start\", \"end\")\n][ # in other column replace NA string with word None\n  official_name_of_order == \"N/A\", official_name_of_order := \"None\"\n][ # add year to dates so we can convert them\n  , c(\"start\", \"end\") := lapply(.SD, function(x) fifelse(!is.na(x), paste(x, \"2020\"), x)),\n  , .SDcols = c(\"start\", \"end\")\n][# then convert all to proper date class\n  , c(\"start\", \"end\") := lapply(.SD, anytime::anydate), .SDcols = c(\"start\", \"end\")\n]\n\nstay_at_home_table[1]\n\n     state            official_name_of_order      start        end\n1: Alabama Suspend certain public gatherings 2020-04-04 2020-04-30\n\n\nGet the COVID-19 data using the fast fread() function from data.table, and also sum up the total cases and deaths and get the date range of the data.\n\n\n# get state data from The New York Times github\nstates <- fread(\n  input = \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\",\n  colClasses = c(date = 'IDate'),\n  key = c('state', 'date')\n)\n\n# get national data from The New York Times github\nus <- fread(\n  input = \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv\",\n  colClasses = c(date = 'IDate'),\n  key = 'date'\n)\n\n# total cases in the US\nus_cases <- us[, last(cases)] %>% \n  formatC(digits = 0, format = \"d\", big.mark = \",\")\n\n# total deaths in the US\nus_deaths <- us[, last(deaths)] %>% \n  formatC(digits = 0, format = \"d\", big.mark = \",\")\n\n# date range of the data\ndate_range <- states[, range(unique(date))]\nnames(date_range) <- c(\"first\", \"last\")\n\n\nBuild our data for the plot. We’ll make 7-day rolling averages for cases and deaths. The rolling averages will smooth over reporting inconsistencies, for example, when a hospital or county may report all weekend deaths on a Monday. Then we will join the state’s closing and reopening dates onto our COVID counts for each state.\n\n\nplot_dat <- states[\n  , `:=`( # get new cases and deaths per day by subtracting previous day (lag)\n      new_cases =  cases - shift(cases,  type = \"lag\", n = 1L),\n      new_deaths = deaths -shift(deaths, type = \"lag\", n = 1L)\n    )\n    , by = state\n  ][ # rolling avergaes for new cases and new deaths by state\n    , `:=`(\n      nc_avg = frollmean(new_cases,  7L, align = \"right\"),\n      nd_avg = frollmean(new_deaths, 7L, align = \"right\")\n    )\n    , by = state\n  ][ # round them all up using ceiling\n    , c(\"nc_avg\", \"nd_avg\") := lapply(.SD, ceiling), .SDcols = c(\"nc_avg\", \"nd_avg\")\n  ][ # join the closing and reopening dates\n    stay_at_home_table, on = \"state\"\n  ][\n    , plot_label_end := \"REOPEN\"\n  ][\n    , plot_label_start := \"CLOSE\"\n  ]\n\n\nPlotting all the data for each state would be too much for this blog post. Instead, I’ll pick two states and plot their daily death counts, 7-day rolling averages, and closing and opening dates.\n\n\n# colors\nmain_blue <- \"#abbfd3\"\naccent_light <- \"#eef2f6\"\naccent_dark <- \"#698cb0\"\ngrey_dark <- \"#627384\"\nrestriction_red <- \"#f47171\"\nreopen_green <- \"#24a8a8\"\n\n# max date for x axis\nmax_plot_date <- max(\n  Sys.Date(),\n  plot_dat[state %chin% c(\"Texas\", \"Michigan\"),\n           max(end, na.rm = TRUE)]\n)\n\nplot_dat[state %chin% c(\"Texas\", \"Michigan\"), ] %>% \n      ggplot(aes(x = date, y = new_deaths)) + \n      geom_bar(stat = \"identity\",\n               fill = main_blue,\n               color = accent_light,\n               width = 1,\n               alpha = .4) +\n      geom_smooth(aes(x = date, y = nd_avg),\n                  method = \"loess\",\n                  color = accent_dark,\n                  se = FALSE,\n                  size = .5) +\n      geom_text(aes(x = as.Date(\"2020-03-11\"), y = 0.01,\n                    label = \"PANDEMIC 3/11\"),\n                size = 2, angle = 90, vjust = 0, hjust = 0, color = grey_dark) +\n      geom_text(aes(x = start, y = 0.01, label = plot_label_start),\n                size = 2, angle = 90, vjust = 0, hjust = 0, color = restriction_red) +\n      geom_text(aes(x = end, y = 0.01, label = plot_label_end),\n               size = 2, angle = 90, vjust = 0, hjust = 0, color = reopen_green) +\n      scale_x_date(limits = c(as.Date(\"2020-03-10\"), max_plot_date), breaks = \"1 week\") +\n      facet_wrap(~state, scales = \"free\",\n                 ncol = 1) +\n      labs(y = \"\",\n           x = \"\",\n           title = \"New Deaths by Day Since March 11th 2020\",\n           subtitle = \"on March 11th, 2020 the WHO declared the coronavirus outbreak a global pandemic\",\n           caption = \"Dataviz by Jeremy Allen | COVID data by The New York Time | Open Close Dates by Ballotpedia\") +\n      theme(\n        axis.ticks = element_line(color = accent_dark, size = .3),\n        axis.line.x = element_line(color = accent_dark, size = .3),\n        axis.text = element_text(color = \"#627384\", size = 8),\n        axis.text.x = element_text(angle = 90, vjust = .5),\n        axis.title = element_text(color = \"#627384\", size = 8),\n        panel.background = element_rect(fill = accent_light, color = accent_light),\n        plot.background = element_rect(fill = accent_light, color = accent_light),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = accent_light, color = accent_light),\n        strip.text = element_text(color = \"#627384\", size = 12, hjust = 0,\n                                  margin = unit(c(10,10,10,10), \"pt\")),\n        plot.title = element_text(color = \"#627384\", size = 20,\n                                  margin = unit(c(2,10,5,0), \"pt\")),\n        plot.subtitle = element_text(color = \"#627384\", size = 14,\n                                     margin = unit(c(5,10,20,0), \"pt\")),\n        plot.caption = element_text(color = \"#627384\", size = 6),\n        plot.margin = unit(c(15,40,15,40), \"pt\"),\n        panel.spacing = unit(70, \"pt\")\n      )\n\n\n\n\n",
    "preview": "posts/2020-05-12-scrape-and-clean-an-online-table/scrape-and-clean-an-online-table_files/figure-html5/plot-1.png",
    "last_modified": "2020-05-13T01:20:47-04:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1152
  },
  {
    "path": "posts/2020-05-11-find-index-of-first-instance/",
    "title": "Find Index of First Instance",
    "description": "Find one thing with another thing. We'll speed test various data.table and tidyverse methods for finding the position of the first match and use that to index another column.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-11",
    "categories": [],
    "contents": "\nYes, another data.table and tidyverse speed test, but this is more than that! I swear. This is real code of me working through a specific real-world issue.\nThe problem: Get the position number of the first instance of a thing in one column, and use that number to pick a thing from another column, in my case, returning the first date on which a specified number of cases occurred.\nLoad packages and make some fake data.\n\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(purrr)\n\n# lots of dates\ndate = seq.Date(from = as.Date(\"1900-01-01\"),\n                to = as.Date(\"2900-12-31\"),\n                by = \"day\")\n\n# lots of cases\ncases = c(1:length(date)-1)\n\n# make a dataframe\ndf <- data.frame(date = date, cases = cases)\n\n\nLet’s use which.max() to get the date on which the first instance of 10,0000 cases occurred.\n\n\n# Get the position of the first instance of 10,000 in the cases col,\n# and use that number to index the date col, returning the first date\n# on which 10,000 cases occurred.\ndt <- as.data.table(df) # convert to data.table first\ndt[, date[which.max(cases >= 10000)]]\n\n[1] \"1927-05-20\"\n\n# this only works because 10000 is a vlaue that can be found in that column.\n\n\nHowever, which.max() returns 1 when it fails, thus indexing our first date, which we do not want because there are no days with 400,000 or more cases.\nWe need NA returned when we don’t find an instance of the value we are looking for.\n\n\n# which.max returns 1 when it fails, thus indexing\n# our first date, which we do not want because there\n# are no days with 400,000 or more cases. We expect NA.\ndt[, date[which.max(cases >= 400000)]]\n\n[1] \"1900-01-01\"\n\n\nLet’s test many methods. We want to speed test them at the end, so I’m putting each method inside a function because it’s easier to add them as functions in the speed test once we get there.\n\n\n# which.max(), does NOT return NA when it fails. Bad.\ndt_which_max_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[which.max(cases >= 400000)]]\n} \n\n# match(true, x) will return NA when it fails, which\n# is what we want so that we don't get a date returned\n# when there are no days with 400,000 or more cases\ndt_match_true_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[match(TRUE, cases >= 100000)]]\n} \n\n# which()[1], test them all and return the first one, also returns NA\ndt_which_first_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[which(cases >= 400000)[1L]]]  \n}\n\n# use base R's Position function, also returns NA\ndt_position_method <- function() {\n  dt <- as.data.table(df)\n  dt[, date[Position(function(x) x >= 400000, cases)]]\n}\n\n# Tidyverse's purrr::detect_index(), returns 'Date of length 0'\ntv_purrr_method <- function() {\n  tb <- tibble::as_tibble(df)\n  tb %>%\n    slice(purrr::detect_index(cases, ~.x >= 400000)) %>% \n    pull(date)\n}\n\n# Tidyverse mixed with the base R's match function\ntv_match_method <- function() {\n  tb <- tibble::as_tibble(df)\n  tb %>%\n    slice(match(TRUE, cases >= 100000)) %>% \n    pull(date)\n}\n\n\nGet each function into microbenchmark and test each one 100 times.\n\n\n#--- Speed test them each 100 times\n\nmicrobenchmark::microbenchmark(\n  dt_which_max_method(),\n  dt_match_true_method(),\n  dt_which_first_method(),\n  dt_position_method(),\n  tv_purrr_method(),\n  tv_match_method(),\n  times = 100L\n)\n\nUnit: milliseconds\n                    expr        min         lq       mean     median\n   dt_which_max_method()   1.945617   2.426501   3.481390   2.705386\n  dt_match_true_method()   1.822808   2.400708   3.650070   2.729901\n dt_which_first_method()   1.945631   2.366789   3.678050   2.605177\n    dt_position_method() 160.590374 178.364530 188.521428 184.895142\n       tv_purrr_method() 785.695242 901.754074 947.888644 931.474398\n       tv_match_method()   1.444482   1.897420   3.139463   2.094831\n         uq        max neval\n   3.306638   10.57839   100\n   3.797825   16.06448   100\n   4.860582   10.31487   100\n 194.051222  265.28539   100\n 977.642047 1403.64966   100\n   2.812000   67.40537   100\n\n\nThe vectorized methods, such as match() used on either a data.table or a tidyverse tibble are clear winners over the base Position() and purrr detect_index() functions.\n\n\n",
    "preview": "posts/2020-05-11-find-index-of-first-instance/index_benchmark.png",
    "last_modified": "2020-05-12T22:20:42-04:00",
    "input_file": {},
    "preview_width": 1514,
    "preview_height": 328
  },
  {
    "path": "posts/2020-05-01-order-months/",
    "title": "Order Months",
    "description": "Produce a vector of month names ending with the current month.",
    "author": [
      {
        "name": "Jeremy Allen",
        "url": "https://jeremydata.com"
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\nThe last 12 months with order_months()\nI frequently have to make tables or plots of events that “happened in the last 12 months.” I like to generate a character vector of month names for the last 12 months which I can then use for ordered factor levels, factor labels, column names, or x-axis labels.\nThis helper function will produce a character vector of names of the last 12 months ending either at the current month or a given month. The current month or given month will be the last element in the vector, with the preceding 11 months before it.\nIf no month number is given for x, the current month is used. Given months must be given as a number. Month names can be returned as abbreviated or full names. Set label = \"abb\" or label = \"names\". X must be 1-12.\nRemember, the current or given month is always at the end of the vector!\n\n\norder_months <- function(x = NULL, label = \"abb\") {\n  \n  # This function takes a given month number or the current month\n  # number and returns a character vector of the last 12 months,\n  # including current month. For example, if it is now February: \n  # \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\" \"Jan\" \"Feb\"\n  # is returned with the current month at the end.\n  # This makes a nice x axis if you need to plot something for\n  # the \"last 12 months\".\n  # if x is NULL the current month is taken from Sys.Date\n  # if x is given, x must be 1-12\n  # if label is \"abb\", month names are abbreviated\n  # if label is \"names\", month names are full\n  \n  # stop if x is out of bounds\n  if(!is.null(x))\n    if(x > 12 | x < 1) stop(\"x must be 1-12\")\n  \n  # sets of named integers using abbreviated and full month names\n  months_abb <- setNames(1:12, month.abb)\n  months_names <- setNames(1:12, month.name)\n  \n  # current month number\n  m <- as.POSIXlt(Sys.Date())$mon + 1\n  \n  # get a value for x\n  if(is.null(x)) x <- m + 1 else x <- x + 1\n  \n  # a and b components for main if\n  # if x is 1 or 12\n  a <- 1:12\n  # if x is 2 through 12\n  b <- c(\n    x:12,\n    1:(x-1)\n    )\n  \n  # main if\n  if(x == 1 | x == 13) new_order <- a else new_order <- b\n\n  # use new_order to set desired order of months\n  if(label == \"abb\") my_month_order <- names(months_abb[new_order])\n  if(label == \"names\") my_month_order <- names(months_names[new_order])\n  \n  my_month_order\n}\n\nTypical usage\n\n\norder_months()\n\n [1] \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\" \"Jan\" \"Feb\" \"Mar\" \"Apr\"\n[12] \"May\"\n\n\n\norder_months(label = \"names\")\n\n [1] \"June\"      \"July\"      \"August\"    \"September\" \"October\"  \n [6] \"November\"  \"December\"  \"January\"   \"February\"  \"March\"    \n[11] \"April\"     \"May\"      \n\n\n\norder_months(x = 5, label = \"abb\")\n\n [1] \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\" \"Jan\" \"Feb\" \"Mar\" \"Apr\"\n[12] \"May\"\n\n\n\n",
    "preview": "posts/2020-05-01-order-months/order_months.png",
    "last_modified": "2020-05-12T17:01:29-04:00",
    "input_file": {},
    "preview_width": 1366,
    "preview_height": 332
  }
]
